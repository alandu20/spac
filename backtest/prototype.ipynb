{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from datetime import datetime as dt\n",
    "from datetime import timedelta\n",
    "from edgar import Company\n",
    "import graphviz\n",
    "import json\n",
    "from lxml import html\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pandas_datareader as pdr\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import re\n",
    "import requests\n",
    "import sec_scraper\n",
    "import statsmodels.api as sm\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import ElasticNet, LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, TimeSeriesSplit, train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "import warnings\n",
    "from yellowbrick.regressor import ResidualsPlot\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_colwidth = 100\n",
    "plotly.offline.init_notebook_mode()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for getting the CIK and Company Name from a SPAC ticker\n",
    "\n",
    "def get_ticker_to_cik(write=False):\n",
    "    # local copy: data/ticker_to_cik.txt\n",
    "    ticker_to_cik = pd.read_csv('https://www.sec.gov/include/ticker.txt',\n",
    "                                sep='\\t', header=None, names=['ticker','cik'])\n",
    "    if write:\n",
    "        ticker_to_cik.to_csv('~/data/ticker_to_cik.csv', index=False)\n",
    "    ticker_to_cik['ticker'] = ticker_to_cik.ticker.str.upper()\n",
    "    ticker_to_cik['cik'] = ticker_to_cik.cik.astype(str)\n",
    "    return ticker_to_cik\n",
    "\n",
    "def get_cik_to_name(write=False):\n",
    "    # local copy: data/cik_to_name.json\n",
    "    cik_to_name = requests.get('https://www.sec.gov/files/company_tickers.json')\n",
    "    cik_to_name = json.dumps(cik_to_name.json())\n",
    "    cik_to_name = pd.read_json(cik_to_name).transpose()\n",
    "    cik_to_name['ticker'] = cik_to_name['ticker'].str.upper()\n",
    "    cik_to_name['cik_str'] = cik_to_name['cik_str'].astype(str)\n",
    "    cik_to_name.rename(columns={'cik_str':'cik'}, inplace=True)\n",
    "    return cik_to_name\n",
    "\n",
    "def get_current_spacs(file_path_current, write=False):\n",
    "    # existing current spac list\n",
    "    df_spacs_existing = pd.read_csv(file_path_current)\n",
    "    df_spacs_existing.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # \"spac traq\" spac list\n",
    "#     df_traq = pd.read_csv('https://docs.google.com/spreadsheets/d/14BY8snyHMbUReQVRgZxv4U4l1ak79dWFIymhrLbQpSo/'\n",
    "#                              'export?gid=0&format=csv', header=2)\n",
    "#     df_traq.columns = [x.replace('\\n','') for x in df_traq.columns]\n",
    "#     spac_traq_symbols = df_traq['Issuer Symbol']\n",
    "    \n",
    "    # \"spac track\" spac list\n",
    "    path_spactrack = 'https://sheet2site.com/api/v3/index.php?key=1F7gLiGZP_F4tZgQXgEhsHMqlgqdSds3vO0-4hoL6ROQ&g=1&e=1&g=1'\n",
    "    page = requests.get(path_spactrack)\n",
    "    tree = html.fromstring(page.content)\n",
    "    html_table = tree.xpath('//table[@class=\"table table-sm\"]')\n",
    "    str_table = html.etree.tostring(html_table[0])\n",
    "    df_track = pd.read_html(str_table)[0]\n",
    "    df_track = df_track[df_track['Status-Filter']!='Pre IPO']\n",
    "    spac_track_symbols = df_track['SPAC Ticker-Filter']\n",
    "    \n",
    "    # combine\n",
    "    combined_spacs = df_spacs_existing['Ticker'].append(spac_track_symbols)\n",
    "    df_spacs_new = pd.DataFrame(combined_spacs, columns=['Ticker'])\n",
    "    df_spacs_new.drop_duplicates(inplace=True)\n",
    "    df_spacs_new.reset_index(inplace=True, drop=True)\n",
    "    new_added_tickers = [x for x in df_spacs_new['Ticker'].tolist()\n",
    "                         if x not in df_spacs_existing['Ticker'].tolist()]\n",
    "    if len(new_added_tickers) > 0:\n",
    "        print('newly added tickers:', new_added_tickers)\n",
    "    if write:\n",
    "        df_spacs_new.to_csv(file_path_current, index=False)\n",
    "    return\n",
    "\n",
    "def process_current_spacs(file_path_current, write=False):\n",
    "    # current spac list\n",
    "    spac_list_current = pd.read_csv(file_path_current)\n",
    "    spac_list_current = spac_list_current.Ticker.unique()\n",
    "    spac_list_current = pd.DataFrame(spac_list_current, columns=['Ticker'])\n",
    "    \n",
    "    # write to file\n",
    "    if write==True:\n",
    "        spac_list_current.to_csv('spac_list_current.csv', index=False)\n",
    "        \n",
    "    # get ticker to cik and cik to company name file, then merge\n",
    "    ticker_to_cik = get_ticker_to_cik(write=write)\n",
    "    cik_to_name = get_cik_to_name(write=write)\n",
    "    spac_list_current = spac_list_current.merge(ticker_to_cik, how='left', left_on='Ticker', right_on='ticker')\n",
    "    spac_list_current = spac_list_current.merge(cik_to_name[['cik','ticker','title']], how='left', on=['cik','ticker'])\n",
    "    \n",
    "    # some current spacs have not split from units to stock + warrants so ticker in sec different\n",
    "    ticker_unit = pd.DataFrame(spac_list_current[spac_list_current.ticker.isna()]['Ticker'])\n",
    "    ticker_unit['Ticker'] = ticker_unit.Ticker + 'U'\n",
    "    ticker_unit = ticker_unit.merge(ticker_to_cik, how='left', left_on='Ticker', right_on='ticker')\n",
    "    ticker_unit = ticker_unit.merge(cik_to_name[['cik','ticker','title']], how='left', on=['cik','ticker'])\n",
    "    ticker_unit['Ticker'] = ticker_unit.Ticker.apply(lambda x: x[:-1] if not pd.isnull(x) else x)\n",
    "    ticker_unit['ticker'] = ticker_unit.ticker.apply(lambda x: x[:-1] if not pd.isnull(x) else x)\n",
    "    ticker_unit_other = pd.DataFrame(ticker_unit[ticker_unit.ticker.isna()]['Ticker'])\n",
    "    ticker_unit_other['Ticker'] = ticker_unit_other.Ticker.apply(lambda x: x+'-UN')\n",
    "    ticker_unit_other = ticker_unit_other.merge(ticker_to_cik, how='left', left_on='Ticker', right_on='ticker')\n",
    "    ticker_unit_other = ticker_unit_other.merge(cik_to_name[['cik','ticker','title']], how='left', on=['cik','ticker'])\n",
    "    ticker_unit_other['Ticker'] = ticker_unit_other.Ticker.apply(lambda x: x[:-3] if not pd.isnull(x) else x)\n",
    "    ticker_unit_other['ticker'] = ticker_unit_other.ticker.apply(lambda x: x[:-3] if not pd.isnull(x) else x)\n",
    "    ticker_unit = ticker_unit.append(ticker_unit_other)\n",
    "    ticker_unit.dropna(inplace=True)\n",
    "    \n",
    "    # append tickers found using 'U' and '-UN'\n",
    "    spac_list_current = spac_list_current[~spac_list_current.Ticker.isin(ticker_unit.Ticker)]\n",
    "    spac_list_current = spac_list_current.append(ticker_unit)\n",
    "    \n",
    "    print('count current spacs:', len(spac_list_current))\n",
    "    print('count nan in current spacs:', len(spac_list_current[spac_list_current.ticker.isna()]))\n",
    "    \n",
    "    return spac_list_current\n",
    "    \n",
    "def process_past_spacs(file_path_past, write=False):\n",
    "    # past spac list (completed business combination)\n",
    "    spac_list_past = pd.read_csv(file_path_past)\n",
    "    spac_list_past.fillna('missing', inplace=True)\n",
    "    spac_list_past['dupe_filter'] = spac_list_past['Old Ticker'] + spac_list_past['New Ticker']\n",
    "    spac_list_past = spac_list_past[spac_list_past.dupe_filter.isin(spac_list_past.dupe_filter.unique())]\n",
    "    spac_list_past.drop(columns=['dupe_filter'], inplace=True)\n",
    "    \n",
    "    # write to file\n",
    "    if write==True:\n",
    "        spac_list_past.to_csv('spac_list_past.csv', index=False)\n",
    "        \n",
    "    # get ticker to cik and cik to company name file, then merge\n",
    "    ticker_to_cik = get_ticker_to_cik(write=write)\n",
    "    cik_to_name = get_cik_to_name(write=write)\n",
    "    spac_list_past = spac_list_past.merge(ticker_to_cik, how='left', left_on='New Ticker', right_on='ticker')\n",
    "    spac_list_past = spac_list_past.merge(ticker_to_cik, how='left', left_on='Old Ticker', right_on='ticker')\n",
    "    spac_list_past.rename(columns={'ticker_x':'ticker','cik_x':'cik','cik_y':'cik_old'}, inplace=True)\n",
    "    spac_list_past.drop(columns='ticker_y', inplace=True)\n",
    "    spac_list_past = spac_list_past.merge(cik_to_name[['cik','ticker','title']], how='left', on=['cik','ticker'])\n",
    "\n",
    "    print('count past spacs:', len(spac_list_past))\n",
    "    print('count nan in past spacs:', len(spac_list_past[spac_list_past.ticker.isna()]))\n",
    "    \n",
    "    return spac_list_past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for pulling 8-K html, converting to text, grabbing date, and simple text match classifier\n",
    "\n",
    "def basic_text_cleaning(text):\n",
    "    text = text.replace('\\n',' ').replace('\\xa0',' ') # replace some unicode characters\n",
    "    text = re.sub(' +', ' ', text) # remove extra spaces\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def get_forms_text(company_name, cik_id, form_type):\n",
    "    c = sec_scraper.Company(company_name, cik_id, timeout=30)\n",
    "    filings = c.get_all_filings(filing_type=form_type, no_of_documents=100)\n",
    "    dates = [f.accepted_date.strftime('%Y-%m-%d %H:%M:%S') for f in filings]\n",
    "    documents = [basic_text_cleaning(f.documents[0]) for f in filings]\n",
    "    urls = [f.url for f in filings]\n",
    "    print(company_name, cik_id)\n",
    "    df = pd.DataFrame(list(zip(dates, documents, urls)), columns=['date','text','url'])\n",
    "    df['form'] = '8-K'\n",
    "    df['accepted_time'] = df.date\n",
    "    df['date'] = df.date.apply(lambda x: x[0:10])\n",
    "    df['url'] = df.url\n",
    "    return df\n",
    "\n",
    "def basic_text_match(df_form, substring):\n",
    "    df_form[substring.replace(' ','_')+'_found'] = df_form.text.apply(lambda x: 1 if substring in x else 0)\n",
    "    return df_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for grabbing and saving price data\n",
    "\n",
    "def bulk_save_prices_to_csv(symbols, start_date, end_date, price_source, is_warrant=False):\n",
    "    for ind, symbol in enumerate(symbols):\n",
    "        print(symbol)\n",
    "        df_prices = get_historical_prices(symbol=symbol,\n",
    "                                          start_date=start_date,\n",
    "                                          end_date=end_date,\n",
    "                                          price_source=price_source,\n",
    "                                          is_warrant=is_warrant)\n",
    "        if df_prices is not None:\n",
    "            df_prices = process_historical_prices(df_prices)\n",
    "            df_prices = df_prices[[col for col in df_prices.columns if '%chg' not in col]]\n",
    "            df_prices.to_csv('~/data/prices_'+price_source+'/daily_data/'+symbol+'_prices.csv', index=False)\n",
    "        # TD and alphavantage limits query rate\n",
    "        if ind%10==0 and ind!=0:\n",
    "            time.sleep(30)\n",
    "        \n",
    "def run_bulk_save_daily_prices(spac_list_current, spac_list_past, price_source, is_warrant=False):\n",
    "    # spacs missing price data entirely\n",
    "    missing_past_spacs = ['missing', 'LCAH', 'FMCI1', 'CFCO']\n",
    "    missing_current_spacs = ['ACNDU', 'BRLI', 'DFHT', 'DMYD', 'GOAC', 'IWAC',\n",
    "                             'LCAH', 'MCAC', 'PANA', 'PSAC', 'PSTH', 'SSMC']\n",
    "    \n",
    "    symbols_past_new_ticker = [x for x in spac_list_past.ticker.unique().tolist() if str(x)!='nan']\n",
    "    symbols_past_old_ticker = spac_list_past['Old Ticker'].unique().tolist()\n",
    "    symbols_past_old_ticker = [x for x in symbols_past_old_ticker if x not in missing_past_spacs]\n",
    "    symbols_current = [x for x in spac_list_current['Ticker'] if x not in missing_current_spacs]\n",
    "    if is_warrant:\n",
    "        symbols_past_new_ticker = [x + 'W' for x in symbols_past_new_ticker]\n",
    "        symbols_past_old_ticker = [x + 'W' for x in symbols_past_old_ticker]\n",
    "        symbols_current = [x + 'W' for x in symbols_current]\n",
    "    bulk_save_prices_to_csv(symbols=symbols_past_new_ticker, start_date='2018-01-01',\n",
    "                            end_date=dt.today().strftime('%Y-%m-%d'), price_source=price_source, is_warrant=is_warrant)\n",
    "    bulk_save_prices_to_csv(symbols=symbols_past_old_ticker, start_date='2018-01-01',\n",
    "                            end_date=dt.today().strftime('%Y-%m-%d'), price_source=price_source, is_warrant=is_warrant)\n",
    "    bulk_save_prices_to_csv(symbols=symbols_current, start_date='2018-01-01',\n",
    "                            end_date=dt.today().strftime('%Y-%m-%d'), price_source=price_source, is_warrant=is_warrant)\n",
    "\n",
    "def get_td_key_locally():\n",
    "    f = open(\"~/data/td_consumer_key.txt\", \"r\")\n",
    "    return f.read()\n",
    "\n",
    "def millis_since_epoch_to_dt(x):\n",
    "    return dt.fromtimestamp(float(x)/1000).strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "def dt_to_millis_since_epoch(x):\n",
    "    epoch = dt.utcfromtimestamp(0)\n",
    "    return str(int((x-epoch).total_seconds()*1000))\n",
    "\n",
    "def get_prices_td(symbol, start_date, end_date, minute_freq=False):\n",
    "    TD_CONSUMER_KEY = get_td_key_locally()\n",
    "    start_date_epoch = dt_to_millis_since_epoch(dt.strptime(start_date, '%Y-%m-%d'))\n",
    "    end_date_epoch = dt_to_millis_since_epoch(dt.strptime(end_date, '%Y-%m-%d'))\n",
    "    partial_url = 'https://api.tdameritrade.com/v1/marketdata/{symbol}/pricehistory?'\n",
    "    # minute-to-minute prices (<2 months available)\n",
    "    if minute_freq:\n",
    "        endpoint = partial_url+'periodType=day&frequencyType=minute&startDate={start_date}&endDate={end_date}'\n",
    "    # daily open-close prices\n",
    "    else:\n",
    "        endpoint = partial_url+'periodType=month&frequencyType=daily&startDate={start_date}&endDate={end_date}'\n",
    "    full_url = endpoint.format(symbol=symbol,\n",
    "                               start_date=start_date_epoch,\n",
    "                               end_date=end_date_epoch)\n",
    "    page = requests.get(url=full_url, params={'apikey' : TD_CONSUMER_KEY})\n",
    "    content = json.loads(page.content)\n",
    "    try:\n",
    "        df_td_prices = pd.DataFrame.from_dict(content['candles'])\n",
    "    except:\n",
    "        return None\n",
    "    if len(df_td_prices)==0:\n",
    "        return None\n",
    "    df_td_prices['datetime'] = df_td_prices.datetime.apply(lambda x: millis_since_epoch_to_dt(x))\n",
    "    df_td_prices['date'] = df_td_prices.datetime.apply(lambda x: x[0:10])\n",
    "    df_td_prices['ticker'] = symbol\n",
    "    return df_td_prices\n",
    "\n",
    "def get_historical_prices(symbol, start_date, end_date, price_source, is_warrant=False):\n",
    "    print('getting prices from', start_date, 'to', end_date, 'for', symbol)\n",
    "    if price_source=='td':\n",
    "        # get prices using td api\n",
    "        df_prices = get_prices_td(symbol, start_date, end_date)\n",
    "        \n",
    "        # td symbol format for some warrants is ___.WS\n",
    "        if df_prices is None and is_warrant:\n",
    "            df_prices = get_prices_td(symbol[:-1]+'.WS', start_date, end_date)\n",
    "    else:\n",
    "        start_split = start_date.split('-')\n",
    "        end_split = end_date.split('-')\n",
    "        start = dt(int(start_split[0]), int(start_split[1]), int(start_split[2]))\n",
    "        end = dt(int(end_split[0]), int(end_split[1]), int(end_split[2]))\n",
    "        try:\n",
    "            # be careful with missing/limited data in yahoo data\n",
    "            if price_source=='yahoo':\n",
    "                df_prices = pdr.data.DataReader(name=symbol, data_source='yahoo', start=start, end=end)\n",
    "            # alphavantage seems to be most reliable. 5 calls per minute and 500 calls per day\n",
    "            if price_source=='alphavantage':\n",
    "                df_prices = pdr.data.DataReader(name=symbol, data_source='av-daily', start=start, end=end,\n",
    "                                                api_key='BDB9WJQRCZKINCLD')\n",
    "            # iex has extremely limited api calls\n",
    "            if price_source=='iex':\n",
    "                df_prices = pdr.data.DataReader(name=symbol, data_source='iex', start=start, end=end,\n",
    "                                                api_key='pk_970dfff359894b15a056cf677c02e11f')\n",
    "            df_prices.reset_index(inplace=True)\n",
    "            df_prices.rename(columns={'index':'date'}, inplace=True)\n",
    "        except:\n",
    "            return None\n",
    "    return df_prices\n",
    "\n",
    "def process_historical_prices(df_prices, conservative_est=False):\n",
    "    df_prices.columns = df_prices.columns.str.lower()\n",
    "    df_prices['date'] = df_prices.date.astype(str)\n",
    "    \n",
    "    # find price on t+# date (can't use shift operator b/c some (business) days missing\n",
    "    # e.g. ACAMW 2019-04-15->2019-04-16->2019-06-05)\n",
    "    def get_price_date_t(date, t, open_or_close):\n",
    "        date = dt.strptime(date, '%Y-%m-%d')\n",
    "        date_range = [date + timedelta(days=i) for i in range(0,30)]\n",
    "        date_range_no_weekend = [date.strftime('%Y-%m-%d') for date in date_range if date.weekday() not in [5,6]]\n",
    "        date_t = date_range_no_weekend[t]\n",
    "        # assume missing day means no volume that day\n",
    "        if date_t > df_prices.iloc[-1]['date']:\n",
    "            return np.nan\n",
    "        return df_prices[df_prices['date']<=date_t].iloc[-1][open_or_close]\n",
    "    \n",
    "    # add price columns\n",
    "    df_prices['open_t+1'] = df_prices.date.apply(lambda x: get_price_date_t(x, 1, 'open'))\n",
    "    for i in range(1,13):\n",
    "        df_prices['close_t+'+str(i)] = df_prices.date.apply(lambda x: get_price_date_t(x, i, 'close'))\n",
    "        # for conservative estimate, mark return from open_t+1 to close_t+#\n",
    "        if conservative_est:\n",
    "            df_prices['open_close_t+'+str(i)+'_%chg'] = (df_prices['close_t+'+str(i)] - df_prices['open_t+1']) / df_prices['open_t+1']\n",
    "        # if not conservative estimate, mark return from open_t to close_t+#\n",
    "        else:\n",
    "            df_prices['open_close_t+'+str(i)+'_%chg'] = (df_prices['close_t+'+str(i)] - df_prices['open']) / df_prices['open']\n",
    "    df_prices = df_prices.round(2)\n",
    "    print('output min date:', df_prices.date.min())\n",
    "    print('output max date:', df_prices.date.max())\n",
    "    return df_prices\n",
    "\n",
    "# df_prices = pd.read_csv('data/prices_td/daily_data/CCXXW_prices.csv')\n",
    "# df_prices = df_prices[['open','high','low','close','volume','datetime','date','ticker']]\n",
    "# df_prices = process_historical_prices(df_prices, conservative_est=True)\n",
    "# df_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions outputting dataframe of 8-Ks, classification, and 1,3,5,7 day returns\n",
    "\n",
    "def agg_spac_returns(spac_list, price_source, is_warrant=False, write=False, conservative_est=False):\n",
    "    df_returns_agg = pd.DataFrame()\n",
    "    count_missing_8K = 0\n",
    "    for ind in range(0, len(spac_list)):\n",
    "        row = spac_list.iloc[ind]\n",
    "        print('index:', ind)\n",
    "        print(row.ticker)\n",
    "        \n",
    "        broken_current_spacs = ['GNRS','KBLM','LGC','LIVE','NOVS']\n",
    "        if is_warrant:\n",
    "            broken_current_spacs = [x + 'W' for x in broken_current_spacs]\n",
    "        if row.ticker in broken_current_spacs:\n",
    "            print('in broken spac list, skipping...')\n",
    "            continue\n",
    "\n",
    "        # get form 8Ks\n",
    "        df_form_8K = get_forms_text(company_name=row.title, cik_id=row.cik, form_type='8-K')\n",
    "        if df_form_8K is None or len(df_form_8K)==0:\n",
    "            print('no 8Ks found, trying cik_old...')\n",
    "            if 'cik_old' in row.index:\n",
    "                df_form_8K = get_forms_text(company_name=row.title, cik_id=row.cik_old, form_type='8-K')\n",
    "        if df_form_8K is None or len(df_form_8K)==0:\n",
    "            print('no 8Ks found, skipping...\\n')\n",
    "            count_missing_8K = count_missing_8K + 1\n",
    "            continue\n",
    "        else:\n",
    "            if write:\n",
    "                df_form_8K.to_csv('~/data/sec_filings_df/'+row.ticker+'_sec_forms.csv', index=False)\n",
    "            # simple classifier\n",
    "            df_form_8K = basic_text_match(df_form_8K, 'letter of intent')\n",
    "            df_form_8K = basic_text_match(df_form_8K, 'business combination agreement')\n",
    "\n",
    "        # get stock or warrant prices\n",
    "        try:\n",
    "            # load saved data\n",
    "            df_prices = pd.read_csv('~/data/prices_'+price_source+'/daily_data/'+row.ticker+'_prices.csv') \n",
    "            print('price data min date:', df_prices.date.min())\n",
    "            print('price data max date:', df_prices.date.max())\n",
    "        except:\n",
    "            df_prices = get_historical_prices(symbol=row.ticker,\n",
    "                                              start_date='2018-01-01',\n",
    "                                              end_date=dt.today().strftime('%Y-%m-%d'),\n",
    "                                              price_source=price_source,\n",
    "                                              is_warrant=is_warrant)\n",
    "        if df_prices is None:\n",
    "            print('prices for', row.ticker, 'not found. skipping...\\n')\n",
    "            continue\n",
    "        df_prices = process_historical_prices(df_prices, conservative_est)\n",
    "        \n",
    "        # add completion (de-spac) date price if past spac\n",
    "        if 'Completion Date' in spac_list.columns:\n",
    "            if is_warrant:\n",
    "                ticker_match = row.ticker[:-1]\n",
    "            else:\n",
    "                ticker_match = row.ticker\n",
    "            if len(spac_list[spac_list['New Ticker']==ticker_match])!=0:\n",
    "                completion_date = spac_list[spac_list['New Ticker']==ticker_match]['Completion Date'].iloc[0]\n",
    "            elif len(spac_list[spac_list['Old Ticker']==ticker_match])!=0:\n",
    "                completion_date = spac_list[spac_list['Old Ticker']==ticker_match]['Completion Date'].iloc[0]\n",
    "            else:\n",
    "                completion_date = np.nan\n",
    "                print('could not find completion (de-spac) date')\n",
    "            print('completion (de-spac) date:', completion_date)\n",
    "            completion_row = df_prices[df_prices['date']==completion_date]\n",
    "            if len(completion_row)==0 or pd.isna(completion_date):\n",
    "                df_prices['open_completion_%chg'] = np.nan\n",
    "                print('could not find price for completion (de-spac) date')\n",
    "            else:\n",
    "                completion_price = completion_row['close'].iloc[0]\n",
    "                df_prices['completion_price'] = completion_price\n",
    "                print('completion (de-spac) price:', completion_price)\n",
    "                # for conservative estimate, mark return from open_t+1\n",
    "                if conservative_est:\n",
    "                    df_prices['open_completion_%chg'] = (df_prices['completion_price'] - df_prices['open_t+1']) / df_prices['open_t+1']\n",
    "                # if not conservative estimate, mark return from open_t\n",
    "                else:\n",
    "                    df_prices['open_completion_%chg'] = (df_prices['completion_price'] - df_prices['open']) / df_prices['open']  \n",
    "                # set to nan if date >= completion_date\n",
    "                df_prices['open_completion_%chg'] = np.where(df_prices['date']>=completion_date, np.nan, np.round(df_prices['open_completion_%chg'], 2))\n",
    "        else:\n",
    "            df_prices['open_completion_%chg'] = np.nan\n",
    "\n",
    "        # merge sec forms and price data on date\n",
    "        cols = ['date']\n",
    "        cols_pct_chg = [col for col in df_prices.columns if '%chg' in col]\n",
    "        cols.extend(cols_pct_chg)\n",
    "        df_returns = df_form_8K.merge(df_prices[cols], how='left', on='date') # some dates missing\n",
    "        display(df_returns)\n",
    "\n",
    "        # append\n",
    "        if len(df_returns)!=0:\n",
    "            df_returns['symbol'] = row.ticker\n",
    "        df_returns_agg = df_returns_agg.append(df_returns)\n",
    "        \n",
    "        # sec site sometimes will timeout\n",
    "        if ind%10==0 and ind!=0:\n",
    "            time.sleep(30) # set to at least 120 loading sec forms from scratch\n",
    "        \n",
    "    # drop duplicates on date + symbol\n",
    "    # todo: if date has multipled 8-Ks, concatenate instead of dropping\n",
    "    print('count sec forms before dropping duplicates:', len(df_returns_agg))\n",
    "    df_returns_agg = df_returns_agg.drop_duplicates(subset=['date','symbol'])\n",
    "    print('count sec forms after dropping duplicates:', len(df_returns_agg))\n",
    "    \n",
    "    print('\\n##############\\ncount symbols missing 8-Ks:', count_missing_8K, '\\n##############\\n')\n",
    "    \n",
    "    return df_returns_agg\n",
    "\n",
    "def load_all_spacs(use_saved_df=True, write=False, warrants_only=False, conservative_est=False):\n",
    "    if conservative_est:\n",
    "        path_returns = '~/data/returns/conservative/'\n",
    "    else:\n",
    "        path_returns = '~/data/returns/'\n",
    "    if use_saved_df:\n",
    "        df_returns_past_warrants_newticker = pd.read_csv(path_returns+'df_returns_past_warrants_newticker.csv')\n",
    "        df_returns_past_warrants_oldticker = pd.read_csv(path_returns+'df_returns_past_warrants_oldticker.csv')\n",
    "        df_returns_current_warrants = pd.read_csv(path_returns+'df_returns_current_warrants.csv')\n",
    "        if warrants_only:\n",
    "            return df_returns_past_warrants_newticker, df_returns_past_warrants_oldticker, df_returns_current_warrants\n",
    "        df_returns_past = pd.read_csv(path_returns+'df_returns_past.csv')\n",
    "        df_returns_current = pd.read_csv(path_returns+'df_returns_current.csv')\n",
    "        return df_returns_past, df_returns_current, df_returns_past_warrants_newticker, df_returns_past_warrants_oldticker, df_returns_current_warrants\n",
    "    else:\n",
    "        # get returns following 8-Ks for past spac warrants (new and old tickers)\n",
    "        spac_list_past_warrants_newticker = spac_list_past.copy()\n",
    "        spac_list_past_warrants_newticker['ticker'] = spac_list_past_warrants_newticker['New Ticker'] + 'W' # warrants\n",
    "        df_returns_past_warrants_newticker = agg_spac_returns(spac_list_past_warrants_newticker,\n",
    "                                                              price_source='td',\n",
    "                                                              is_warrant=True,\n",
    "                                                              write=write,\n",
    "                                                              conservative_est=conservative_est)\n",
    "        spac_list_past_warrants_oldticker = spac_list_past.copy()\n",
    "        spac_list_past_warrants_oldticker['ticker'] = spac_list_past_warrants_oldticker['Old Ticker'] + 'W' # warrants\n",
    "        df_returns_past_warrants_oldticker = agg_spac_returns(spac_list_past_warrants_oldticker,\n",
    "                                                              price_source='td',\n",
    "                                                              is_warrant=True,\n",
    "                                                              write=write,\n",
    "                                                              conservative_est=conservative_est)\n",
    "\n",
    "        # get returns following 8-Ks for current spac warrants\n",
    "        spac_list_current_warrants = spac_list_current.copy()\n",
    "        spac_list_current_warrants['ticker'] = spac_list_current_warrants.Ticker + 'W' # warrants\n",
    "        df_returns_current_warrants = agg_spac_returns(spac_list_current_warrants,\n",
    "                                                       price_source='td',\n",
    "                                                       is_warrant=True,\n",
    "                                                       write=write,\n",
    "                                                       conservative_est=conservative_est)\n",
    "        if warrants_only:\n",
    "            return df_returns_past_warrants_newticker, df_returns_past_warrants_oldticker, df_returns_current_warrants\n",
    "        \n",
    "        # get returns following 8-Ks for past spacs\n",
    "        df_returns_past = agg_spac_returns(spac_list_past,\n",
    "                                           price_source='td',\n",
    "                                           is_warrant=False,\n",
    "                                           write=write,\n",
    "                                           conservative_est=conservative_est)\n",
    "\n",
    "        # get returns following 8-Ks for current spacs\n",
    "        df_returns_current = agg_spac_returns(spac_list_current,\n",
    "                                              price_source='td',\n",
    "                                              is_warrant=False,\n",
    "                                              write=write,\n",
    "                                              conservative_est=conservative_est)\n",
    "\n",
    "        if write:\n",
    "            df_returns_past.to_csv(path_returns+'df_returns_past.csv', index=False)\n",
    "            df_returns_current.to_csv(path_returns+'df_returns_current.csv', index=False)\n",
    "            df_returns_past_warrants_newticker.to_csv(path_returns+'df_returns_past_warrants_newticker.csv', index=False)\n",
    "            df_returns_past_warrants_oldticker.to_csv(path_returns+'df_returns_past_warrants_oldticker.csv', index=False)\n",
    "            df_returns_current_warrants.to_csv(path_returns+'df_returns_current_warrants.csv', index=False)\n",
    "            \n",
    "    return df_returns_past, df_returns_current, df_returns_past_warrants_newticker, df_returns_past_warrants_oldticker, df_returns_current_warrants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spac_track_table(path_spactrack):\n",
    "    page = requests.get(path_spactrack)\n",
    "    tree = html.fromstring(page.content)\n",
    "    html_table = tree.xpath('//table[@class=\"table table-sm\"]')\n",
    "    str_table = html.etree.tostring(html_table[0])\n",
    "    df_track = pd.read_html(str_table)[0]\n",
    "    return df_track\n",
    "\n",
    "def process_past_spacs(file_path_past, write=False):\n",
    "    # past spac list (completed business combination)\n",
    "    spac_list_past = pd.read_csv(file_path_past)\n",
    "    spac_list_past.fillna('missing', inplace=True)\n",
    "    spac_list_past['dupe_filter'] = spac_list_past['Old Ticker'] + spac_list_past['New Ticker']\n",
    "    spac_list_past = spac_list_past[spac_list_past.dupe_filter.isin(spac_list_past.dupe_filter.unique())]\n",
    "    spac_list_past.drop(columns=['dupe_filter'], inplace=True)\n",
    "    \n",
    "    # write to file\n",
    "    if write==True:\n",
    "        spac_list_past.to_csv('spac_list_past.csv', index=False)\n",
    "        \n",
    "    # get ticker to cik and cik to company name file, then merge\n",
    "    ticker_to_cik = get_ticker_to_cik(write=write)\n",
    "    cik_to_name = get_cik_to_name(write=write)\n",
    "    spac_list_past = spac_list_past.merge(ticker_to_cik, how='left', left_on='New Ticker', right_on='ticker')\n",
    "    spac_list_past = spac_list_past.merge(ticker_to_cik, how='left', left_on='Old Ticker', right_on='ticker')\n",
    "    spac_list_past.rename(columns={'ticker_x':'ticker','cik_x':'cik','cik_y':'cik_old'}, inplace=True)\n",
    "    spac_list_past.drop(columns='ticker_y', inplace=True)\n",
    "    spac_list_past = spac_list_past.merge(cik_to_name[['cik','ticker','title']], how='left', on=['cik','ticker'])\n",
    "\n",
    "    print('count past spacs:', len(spac_list_past))\n",
    "    print('count nan in past spacs:', len(spac_list_past[spac_list_past.ticker.isna()]))\n",
    "    \n",
    "    # merge \"spac track\" completion date\n",
    "    df_track_past = get_spac_track_table(path_spactrack='https://sheet2site.com/api/v3/index.php?key=1OMSgBXCcW4W-5h20eZYb1RCmkuO6Sf299foDPUF7Fc0&g=1&e=1')\n",
    "    spac_list_past = spac_list_past.merge(df_track_past[['Post-SPAC Ticker Symbol','Completion Date']], how='left', left_on='New Ticker', right_on='Post-SPAC Ticker Symbol')\n",
    "    spac_list_past = spac_list_past[['Old Ticker','New Ticker','Completion Date','Completion Year','ticker','cik','cik_old','title']]\n",
    "    \n",
    "    return spac_list_past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# update current spac list\n",
    "# get_current_spacs(file_path_current='data/spac_list_current.csv', write=True)\n",
    "\n",
    "# load current and past spac lists\n",
    "spac_list_current = process_current_spacs(file_path_current='~/data/spac_list_current.csv', write=False)\n",
    "spac_list_past = process_past_spacs(file_path_past='~/data/spac_list_past.csv', write=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bulk save daily price data (API limits)...run this occasionally to update daily prices\n",
    "# run_bulk_save_daily_prices(spac_list_current, spac_list_past, price_source='td', is_warrant=False)\n",
    "# run_bulk_save_daily_prices(spac_list_current, spac_list_past, price_source='td', is_warrant=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load returns for all spacs (stock and warrants) with returns calculated from open on form accepted date\n",
    "# df_returns_past, df_returns_current, df_returns_past_warrants_newticker, df_returns_past_warrants_oldticker, df_returns_current_warrants = load_all_spacs(use_saved_df=True, write=False, warrants_only=False, conservative_est=False)\n",
    "\n",
    "# load returns for all spacs (warrants only) with returns calculated from open on form accepted date\n",
    "# df_returns_past_warrants_newticker, df_returns_past_warrants_oldticker, df_returns_current_warrants = load_all_spacs(use_saved_df=True, write=False, warrants_only=True, conservative_est=False)\n",
    "\n",
    "# load returns for all spacs (stock and warrants) with returns calculated from open on form accepted date + 1 day (conservative)\n",
    "df_returns_past, df_returns_current, df_returns_past_warrants_newticker, df_returns_past_warrants_oldticker, df_returns_current_warrants = load_all_spacs(use_saved_df=True, write=False, warrants_only=False, conservative_est=True)\n",
    "\n",
    "# load returns for all spacs (warrants only) with returns calculated from open on form accepted date + 1 day (conservative)\n",
    "# df_returns_past_warrants_newticker, df_returns_past_warrants_oldticker, df_returns_current_warrants = load_all_spacs(use_saved_df=True, write=False, warrants_only=True, conservative_est=True)\n",
    "\n",
    "\n",
    "print('count current spacs with 8-Ks found:', len(df_returns_current.symbol.unique()))\n",
    "print('count past spacs with 8-Ks found', len(df_returns_past.symbol.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for backtest\n",
    "# note .123 is 12.3% in output\n",
    "\n",
    "def compute_mean_returns(df_returns, corrupt_symbols, includes_hourly=False):\n",
    "    if len(corrupt_symbols)!=0:\n",
    "        print('Symbols removed:', corrupt_symbols)\n",
    "    if includes_hourly:\n",
    "        agg_dict = {'accepted_time_t+1hr_%chg':'mean','accepted_time_t+3hr_%chg':'mean',\n",
    "                    'accepted_time_t+5hr_%chg':'mean','accepted_time_t+7hr_%chg':'mean',\n",
    "                    'accepted_time_t+10hr_%chg':'mean','accepted_time_t+12hr_%chg':'mean',\n",
    "                    'open_close_t+1_%chg':'mean','open_close_t+3_%chg':'mean',\n",
    "                    'open_close_t+5_%chg':'mean','open_close_t+7_%chg':'mean',\n",
    "                    'open_close_t+10_%chg':'mean','open_close_t+12_%chg':'mean',\n",
    "                    'open_completion_%chg':'mean'}\n",
    "    else:\n",
    "        agg_dict = {'open_close_t+1_%chg':'mean','open_close_t+3_%chg':'mean',\n",
    "                    'open_close_t+5_%chg':'mean','open_close_t+7_%chg':'mean',\n",
    "                    'open_close_t+10_%chg':'mean','open_close_t+12_%chg':'mean',\n",
    "                    'open_completion_%chg':'mean'}\n",
    "    df_mean_returns = df_returns[~df_returns.symbol.isin(corrupt_symbols)].groupby('symbol').agg(agg_dict)\n",
    "    df_mean_returns.dropna(inplace=True)\n",
    "    print('Count:', len(df_mean_returns))\n",
    "    return np.round(df_mean_returns, 3)\n",
    "\n",
    "def summary_statistics(df_returns, corrupt_symbols):\n",
    "    if len(corrupt_symbols)!=0:\n",
    "        print('Symbols removed:', corrupt_symbols)\n",
    "    return np.round(df_returns[~df_returns.symbol.isin(corrupt_symbols)].describe(), 3)\n",
    "\n",
    "def compute_mean_returns_LOI(df_returns, corrupt_symbols, includes_hourly=False):\n",
    "    if len(corrupt_symbols)!=0:\n",
    "        print('Symbols removed:', corrupt_symbols)\n",
    "    if includes_hourly:\n",
    "        agg_dict = {'accepted_time_t+1hr_%chg':'mean','accepted_time_t+3hr_%chg':'mean',\n",
    "                    'accepted_time_t+5hr_%chg':'mean','accepted_time_t+7hr_%chg':'mean',\n",
    "                    'accepted_time_t+10hr_%chg':'mean','accepted_time_t+12hr_%chg':'mean',\n",
    "                    'open_close_t+1_%chg':'mean','open_close_t+3_%chg':'mean',\n",
    "                    'open_close_t+5_%chg':'mean','open_close_t+7_%chg':'mean',\n",
    "                    'open_close_t+10_%chg':'mean','open_close_t+12_%chg':'mean'}\n",
    "    else:\n",
    "        agg_dict = {'open_close_t+1_%chg':'mean','open_close_t+3_%chg':'mean',\n",
    "                    'open_close_t+5_%chg':'mean','open_close_t+7_%chg':'mean',\n",
    "                    'open_close_t+10_%chg':'mean','open_close_t+12_%chg':'mean'}\n",
    "    return np.round(df_returns[(~df_returns.symbol.isin(corrupt_symbols)) &\n",
    "                               (df_returns.letter_of_intent_found==1)].groupby('symbol').agg(agg_dict), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Past SPACs (Stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean returns for past spac stocks')\n",
    "compute_mean_returns(df_returns_past, corrupt_symbols=['ACEL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('summary stats for returns in past spac stocks')\n",
    "summary_statistics(df_returns_past, corrupt_symbols=['ACEL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean returns for past spac stocks (LOI text match)')\n",
    "compute_mean_returns_LOI(df_returns_past, corrupt_symbols=['ACEL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current SPACs (Stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('mean returns for current spac stocks')\n",
    "compute_mean_returns(df_returns_current, corrupt_symbols=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('summary stats for returns in current spac stocks')\n",
    "summary_statistics(df_returns_current, corrupt_symbols=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean returns for current spac stocks (LOI text match)')\n",
    "compute_mean_returns_LOI(df_returns_current, corrupt_symbols=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Past SPAC Warrants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean returns for past spac warrants')\n",
    "compute_mean_returns(df_returns_past_warrants_newticker, corrupt_symbols=['LAZYW'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('summary stats for returns in past spac warrants')\n",
    "summary_statistics(df_returns_past_warrants_newticker, corrupt_symbols=['LAZYW'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean returns for past spac warrants (LOI text match)')\n",
    "compute_mean_returns_LOI(df_returns_past_warrants_newticker, corrupt_symbols=['LAZYW'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current SPAC Warrants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean returns for current spac warrants')\n",
    "compute_mean_returns(df_returns_current_warrants, corrupt_symbols=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('summary stats for returns in current spac warrants')\n",
    "summary_statistics(df_returns_current_warrants, corrupt_symbols=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean returns for current spac warrants (LOI text match)')\n",
    "compute_mean_returns_LOI(df_returns_current_warrants, corrupt_symbols=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_returns_all_warrants = df_returns_past_warrants_newticker.append(df_returns_current_warrants)\n",
    "df_returns_all_warrants = df_returns_all_warrants.replace([np.inf, -np.inf], np.nan)\n",
    "print('mean returns for all spac warrants')\n",
    "compute_mean_returns(df_returns_all_warrants, corrupt_symbols=['LAZYW'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('summary stats for returns all spac warrants')\n",
    "summary_statistics(df_returns_all_warrants, corrupt_symbols=['LAZYW'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results using minute-by-minute data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict_minute_prices(df_returns, min_minute_date='2020-06-01', write=False):\n",
    "    price_dict = dict()\n",
    "    for symbol in df_returns.symbol.unique():\n",
    "        minute_prices = get_prices_td(symbol=symbol, start_date=min_minute_date,\n",
    "                                      end_date=dt.today().strftime('%Y-%m-%d'), minute_freq=True)\n",
    "        if minute_prices is None: # try .WS instead of W\n",
    "            minute_prices = get_prices_td(symbol=symbol[:-1]+'.WS', start_date=min_minute_date,\n",
    "                                          end_date=dt.today().strftime('%Y-%m-%d'), minute_freq=True)\n",
    "        if minute_prices is not None:\n",
    "            # add to dict\n",
    "            minute_prices['datetime'] = minute_prices.datetime.apply(lambda x: x[0:19])\n",
    "            price_dict[symbol] = minute_prices\n",
    "            \n",
    "            # save to file\n",
    "            if write:\n",
    "                path_minute = '~/data/prices_td/minute_data/'+symbol+'.csv'\n",
    "                if os.path.exists(path_minute):\n",
    "                    saved_minute_prices = pd.read_csv(path_minute)\n",
    "                    minute_prices = saved_minute_prices.append(minute_prices)\n",
    "                    minute_prices.drop_duplicates(subset=['datetime'], keep='first', inplace=True)\n",
    "                minute_prices.to_csv(path_minute, index=False)\n",
    "        else:\n",
    "            print(symbol, 'missing')\n",
    "    return price_dict\n",
    "\n",
    "def get_price_at_time(x, n, price_dict, is_hour, return_price=False):\n",
    "    if x.symbol not in price_dict:\n",
    "        return None\n",
    "    symbol_minute_prices = price_dict[x.symbol]\n",
    "    if is_hour:\n",
    "        time_dt = dt.strptime(x.accepted_time, '%Y-%m-%d %H:%M:%S') + timedelta(hours=n)\n",
    "        symbol_minute_prices = symbol_minute_prices[symbol_minute_prices.datetime>time_dt.strftime('%Y-%m-%d %H:%M:%S')]\n",
    "    else:\n",
    "        time_dt = dt.strptime(x.accepted_time, '%Y-%m-%d %H:%M:%S') + timedelta(days=n)\n",
    "        symbol_minute_prices = symbol_minute_prices[symbol_minute_prices.datetime>time_dt.strftime('%Y-%m-%d %H:%M:%S')]\n",
    "    if len(symbol_minute_prices)!=0:\n",
    "        price_at_time = symbol_minute_prices.iloc[0]['open']\n",
    "        if return_price:\n",
    "            return price_at_time\n",
    "        pct_return = (price_at_time - x.accepted_time_price) / x.accepted_time_price\n",
    "        return pct_return\n",
    "    return None\n",
    "\n",
    "def get_returns_with_minute_data(past, current, min_minute_date='2020-06-01', write=False):\n",
    "    # subset all 8-Ks where there exists minute data\n",
    "    df_returns_minute = past.append(current)\n",
    "    df_returns_minute = df_returns_minute[df_returns_minute.date>=min_minute_date]\n",
    "    df_returns_minute.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # create dictionary of symbol, minute prices df\n",
    "    price_dict = create_dict_minute_prices(df_returns=df_returns_minute, min_minute_date=min_minute_date, write=write)\n",
    "\n",
    "    # compute returns\n",
    "    df_returns_minute['accepted_time_price'] = df_returns_minute.apply(lambda x: get_price_at_time(x, price_dict=price_dict, n=0, is_hour=True, return_price=True), axis=1)\n",
    "    for hr in range(1,13):\n",
    "        df_returns_minute['accepted_time_t+'+str(hr)+'hr_%chg'] = df_returns_minute.apply(lambda x: get_price_at_time(x, price_dict=price_dict, n=hr, is_hour=True), axis=1)\n",
    "    for day in range(1,13):\n",
    "        df_returns_minute['accepted_time_t+'+str(day)+'_%chg'] = df_returns_minute.apply(lambda x: get_price_at_time(x, price_dict=price_dict, n=day, is_hour=False), axis=1)\n",
    "\n",
    "    # stats\n",
    "    print('mean returns for current spacs (using minute data)')\n",
    "    display(compute_mean_returns(df_returns_minute, corrupt_symbols=[], includes_hourly=True).sort_values(by='accepted_time_t+1hr_%chg'))\n",
    "\n",
    "    print('summary stats for returns in current spacs (using minute data)')\n",
    "    cols = ['symbol','accepted_time_t+1hr_%chg','accepted_time_t+3hr_%chg','accepted_time_t+5hr_%chg','accepted_time_t+7hr_%chg','accepted_time_t+10hr_%chg','accepted_time_t+12hr_%chg',\n",
    "            'accepted_time_t+1_%chg','accepted_time_t+3_%chg','accepted_time_t+5_%chg','accepted_time_t+7_%chg','accepted_time_t+10_%chg','accepted_time_t+12_%chg']\n",
    "    display(summary_statistics(df_returns_minute[cols], corrupt_symbols=[]))\n",
    "    print(summary_statistics(df_returns_minute[cols], corrupt_symbols=[]).loc['mean'],'\\n')\n",
    "    print(summary_statistics(df_returns_minute[cols], corrupt_symbols=[]).loc['std'],'\\n')\n",
    "\n",
    "    print('mean returns for current spacs (LOI text match) (using minute data)')\n",
    "    display(compute_mean_returns_LOI(df_returns_minute, corrupt_symbols=[], includes_hourly=True).sort_values(by='accepted_time_t+1hr_%chg'))\n",
    "    \n",
    "    return df_returns_minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Stock')\n",
    "df_returns_all_stocks_minute = get_returns_with_minute_data(past=df_returns_past,\n",
    "                                                            current=df_returns_current,\n",
    "                                                            min_minute_date='2020-06-01',\n",
    "                                                            write=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Warrants')\n",
    "df_returns_all_warrants_minute = get_returns_with_minute_data(past=df_returns_past_warrants_newticker,\n",
    "                                                              current=df_returns_current_warrants,\n",
    "                                                              min_minute_date='2020-06-01',\n",
    "                                                              write=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study minute by minute LOI examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_price(symbol, prices):\n",
    "    layout = go.Layout(\n",
    "        title = symbol+' LOI',\n",
    "        xaxis = dict(title='Time'),\n",
    "        yaxis = dict(title='Price'),\n",
    "    )\n",
    "    data = []\n",
    "    trace = go.Scatter(\n",
    "        x = prices.datetime,\n",
    "        y = prices.open,\n",
    "    )\n",
    "    data.append(trace)\n",
    "    fig = dict(data=data, layout=layout)\n",
    "    iplot(fig)\n",
    "    \n",
    "def plot_return(symbol, prices):\n",
    "    layout = go.Layout(\n",
    "        title = symbol+' Return After LOI 8-K Released',\n",
    "        xaxis = dict(title='Time'),\n",
    "        yaxis = dict(title='Return')\n",
    "    )\n",
    "    data = []\n",
    "    trace = go.Scatter(\n",
    "        x = prices.datetime,\n",
    "        y = prices['%return'],\n",
    "    )\n",
    "    data.append(trace)\n",
    "    fig = dict(data=data, layout=layout)\n",
    "    iplot(fig)\n",
    "    \n",
    "def plot_volume(symbol, prices):\n",
    "    layout = go.Layout(\n",
    "        title = symbol+' Volume After LOI 8-K Released',\n",
    "        xaxis = dict(title='Time'),\n",
    "        yaxis = dict(title='Volume')\n",
    "    )\n",
    "    data = []\n",
    "    trace = go.Scatter(\n",
    "        x = prices.datetime,\n",
    "        y = prices['volume'],\n",
    "    )\n",
    "    data.append(trace)\n",
    "    fig = dict(data=data, layout=layout)\n",
    "    iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_returns_current_warrants[(df_returns_current_warrants.symbol=='OPESW') &\n",
    "                                    (df_returns_current_warrants.letter_of_intent_found==1)])\n",
    "accepted_time = '2020-06-09 16:01:19'\n",
    "print('accepted time:', accepted_time)\n",
    "minute_prices = pd.read_csv('~/data/prices_td/minute_data/OPESW.csv')\n",
    "minute_prices = minute_prices[(minute_prices['date']>='2020-06-08') & (minute_prices['date']<='2020-06-13')]\n",
    "minute_prices['datetime'] = minute_prices.datetime.apply(lambda x: x[0:19])\n",
    "plot_price('OPESW', minute_prices)\n",
    "\n",
    "minute_prices_post = minute_prices[minute_prices.datetime>accepted_time]\n",
    "minute_prices_post.reset_index(drop=True, inplace=True)\n",
    "price_before_8K = minute_prices_post.loc[0,'open']\n",
    "minute_prices_post['%return'] = (minute_prices_post.open - price_before_8K) / price_before_8K\n",
    "plot_return('OPESW', minute_prices_post)\n",
    "plot_volume('OPESW', minute_prices_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_returns_current_warrants[(df_returns_current_warrants.symbol=='BMRGW') &\n",
    "                                    (df_returns_current_warrants.letter_of_intent_found==1)])\n",
    "accepted_time = '2020-06-24 13:24:44'\n",
    "print('accepted time:', accepted_time)\n",
    "minute_prices = pd.read_csv('~/data/prices_td/minute_data/BMRGW.csv')\n",
    "minute_prices = minute_prices[(minute_prices['date']>='2020-06-24') & (minute_prices['date']<='2020-06-30')]\n",
    "minute_prices['datetime'] = minute_prices.datetime.apply(lambda x: x[0:19])\n",
    "plot_price('BMRGW', minute_prices)\n",
    "\n",
    "minute_prices_post = minute_prices[minute_prices.datetime>accepted_time]\n",
    "minute_prices_post.reset_index(drop=True, inplace=True)\n",
    "price_before_8K = minute_prices_post.loc[0,'open']\n",
    "minute_prices_post['%return'] = (minute_prices_post.open - price_before_8K) / price_before_8K\n",
    "plot_return('BMRGW', minute_prices_post)\n",
    "plot_volume('BMRGW', minute_prices_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_returns_current_warrants[(df_returns_current_warrants.symbol=='SAMAW') &\n",
    "                                    (df_returns_current_warrants.letter_of_intent_found==1)])\n",
    "accepted_time = '2020-06-01 09:37:22'\n",
    "print('accepted time:', accepted_time)\n",
    "minute_prices = pd.read_csv('~/data/prices_td/minute_data/SAMAW.csv')\n",
    "minute_prices = minute_prices[(minute_prices['date']>='2020-06-01') & (minute_prices['date']<='2020-06-06')]\n",
    "minute_prices['datetime'] = minute_prices.datetime.apply(lambda x: x[0:19])\n",
    "plot_price('SAMAW', minute_prices)\n",
    "\n",
    "minute_prices_post = minute_prices[minute_prices.datetime>accepted_time]\n",
    "minute_prices_post.reset_index(drop=True, inplace=True)\n",
    "price_before_8K = minute_prices_post.loc[0,'open']\n",
    "minute_prices_post['%return'] = (minute_prices_post.open - price_before_8K) / price_before_8K\n",
    "plot_return('SAMAW', minute_prices_post)\n",
    "plot_volume('SAMAW', minute_prices_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_returns_current_warrants[(df_returns_current_warrants.symbol=='NFINW') &\n",
    "                                    (df_returns_current_warrants.letter_of_intent_found==1)])\n",
    "accepted_time = '2020-06-29 09:00:22'\n",
    "print('accepted time:', accepted_time)\n",
    "minute_prices = pd.read_csv('~/data/prices_td/minute_data/NFINW.csv')\n",
    "minute_prices = minute_prices[(minute_prices['date']>='2020-06-29') & (minute_prices['date']<='2020-07-03')]\n",
    "minute_prices['datetime'] = minute_prices.datetime.apply(lambda x: x[0:19])\n",
    "plot_price('NFINW', minute_prices)\n",
    "\n",
    "minute_prices_post = minute_prices[minute_prices.datetime>accepted_time]\n",
    "minute_prices_post.reset_index(drop=True, inplace=True)\n",
    "price_before_8K = minute_prices_post.loc[0,'open']\n",
    "minute_prices_post['%return'] = (minute_prices_post.open - price_before_8K) / price_before_8K\n",
    "plot_return('NFINW', minute_prices_post)\n",
    "plot_volume('NFINW', minute_prices_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_returns_all = df_returns_past.append(df_returns_current)\n",
    "df_returns_all.sort_values(by='date', inplace=True)\n",
    "df_returns_all.set_index('date')['open_close_t+1_%chg'].cumsum().plot(label='Open-Close T+1')\n",
    "df_returns_all.set_index('date')['open_close_t+3_%chg'].cumsum().plot(label='Open-Close T+3')\n",
    "df_returns_all.set_index('date')['open_close_t+5_%chg'].cumsum().plot(label='Open-Close T+5')\n",
    "df_returns_all.set_index('date')['open_close_t+7_%chg'].cumsum().plot(label='Open-Close T+7')\n",
    "df_returns_all.set_index('date')['open_close_t+10_%chg'].cumsum().plot(label='Open-Close T+10')\n",
    "df_returns_all.set_index('date')['open_close_t+12_%chg'].cumsum().plot(label='Open-Close T+12')\n",
    "df_returns_all.set_index('date')['open_completion_%chg'].cumsum().plot(label='Open-Completion')\n",
    "plt.title('Cumulative Return on All 8-Ks, All SPAC Stocks');\n",
    "plt.ylabel('Cumulative Return in Percentage Points (10 = 1000%)');\n",
    "plt.xticks(rotation=45);\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_returns_all = df_returns_past.append(df_returns_current)\n",
    "df_returns_all.sort_values(by='date', inplace=True)\n",
    "plot_col_names = ['open_close_t+1_%chg', 'open_close_t+3_%chg', 'open_close_t+5_%chg', 'open_close_t+7_%chg',\n",
    "                  'open_close_t+10_%chg', 'open_close_t+12_%chg', 'open_completion_%chg']\n",
    "df_cumsum = df_returns_all.set_index('date')[plot_col_names].cumsum()\n",
    "min_date = df_cumsum[~df_cumsum['open_close_t+1_%chg'].isna()].index.min()\n",
    "df_cumsum = df_cumsum[df_cumsum.index>=min_date]\n",
    "fig = go.Figure()\n",
    "fig.update_layout(title='Cumulative Return Trading on All 8-Ks: SPAC Stock',\n",
    "                  xaxis_title='Date',\n",
    "                  yaxis_title='Cumulative Return in Percentage Points (10 = 1000%)')\n",
    "for plot_col_name in plot_col_names:\n",
    "    fig.add_trace(go.Scatter(x=df_cumsum.index,\n",
    "                             y=df_cumsum[plot_col_name],\n",
    "                             mode='lines+markers',\n",
    "                             marker=dict(size=3),\n",
    "                             name=plot_col_name))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_returns_all_stocks_minute.sort_values(by='accepted_time', inplace=True)\n",
    "plt.figure(figsize=(14,6));\n",
    "# df_returns_all_stocks_minute.set_index('accepted_time')['accepted_time_t+1hr_%chg'].cumsum().plot(label='accepted_time+1hr')\n",
    "# df_returns_all_stocks_minute.set_index('accepted_time')['accepted_time_t+3hr_%chg'].cumsum().plot(label='accepted_time+3hr')\n",
    "# df_returns_all_stocks_minute.set_index('accepted_time')['accepted_time_t+5hr_%chg'].cumsum().plot(label='accepted_time+5hr')\n",
    "# df_returns_all_stocks_minute.set_index('accepted_time')['accepted_time_t+7hr_%chg'].cumsum().plot(label='accepted_time+7hr')\n",
    "# df_returns_all_stocks_minute.set_index('accepted_time')['accepted_time_t+10hr_%chg'].cumsum().plot(label='accepted_time+10hr')\n",
    "# df_returns_all_stocks_minute.set_index('accepted_time')['accepted_time_t+12hr_%chg'].cumsum().plot(label='accepted_time+12hr')\n",
    "df_returns_all_stocks_minute.set_index('date')['accepted_time_t+1_%chg'].cumsum().plot(label='accepted_time+1day')\n",
    "df_returns_all_stocks_minute.set_index('date')['accepted_time_t+3_%chg'].cumsum().plot(label='accepted_time+3day')\n",
    "df_returns_all_stocks_minute.set_index('date')['accepted_time_t+5_%chg'].cumsum().plot(label='accepted_time+5day')\n",
    "df_returns_all_stocks_minute.set_index('date')['accepted_time_t+7_%chg'].cumsum().plot(label='accepted_time+7day')\n",
    "df_returns_all_stocks_minute.set_index('date')['accepted_time_t+10_%chg'].cumsum().plot(label='accepted_time+10day')\n",
    "df_returns_all_stocks_minute.set_index('date')['accepted_time_t+12_%chg'].cumsum().plot(label='accepted_time+12day')\n",
    "plt.title('Cumulative Return on All 8-Ks with Minute Data, All SPAC Stocks');\n",
    "plt.ylabel('Cumulative Return in Percentage Points (10 = 1000%)');\n",
    "plt.xticks(rotation=45);\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_returns_all_warrants = df_returns_past_warrants_newticker.append(df_returns_current_warrants)\n",
    "df_returns_all_warrants = df_returns_all_warrants.replace([np.inf, -np.inf], np.nan)\n",
    "df_returns_all_warrants.sort_values(by='date', inplace=True)\n",
    "df_returns_all_warrants.set_index('date')['open_close_t+1_%chg'].cumsum().plot(label='Open-Close T+1')\n",
    "df_returns_all_warrants.set_index('date')['open_close_t+3_%chg'].cumsum().plot(label='Open-Close T+3')\n",
    "df_returns_all_warrants.set_index('date')['open_close_t+5_%chg'].cumsum().plot(label='Open-Close T+5')\n",
    "df_returns_all_warrants.set_index('date')['open_close_t+7_%chg'].cumsum().plot(label='Open-Close T+7')\n",
    "df_returns_all_warrants.set_index('date')['open_close_t+10_%chg'].cumsum().plot(label='Open-Close T+10')\n",
    "df_returns_all_warrants.set_index('date')['open_close_t+12_%chg'].cumsum().plot(label='Open-Close T+12')\n",
    "df_returns_all_warrants.set_index('date')['open_completion_%chg'].cumsum().plot(label='Open-Completion')\n",
    "plt.title('Cumulative Return on All 8-Ks, All SPAC Warrants');\n",
    "plt.ylabel('Cumulative Return in Percentage Points (10 = 1000%)');\n",
    "plt.xticks(rotation=45);\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_returns_all_warrants = df_returns_past_warrants_newticker.append(df_returns_current_warrants)\n",
    "df_returns_all_warrants = df_returns_all_warrants.replace([np.inf, -np.inf], np.nan)\n",
    "df_returns_all_warrants.sort_values(by='date', inplace=True)\n",
    "plot_col_names = ['open_close_t+1_%chg', 'open_close_t+3_%chg', 'open_close_t+5_%chg', 'open_close_t+7_%chg',\n",
    "                  'open_close_t+10_%chg', 'open_close_t+12_%chg', 'open_completion_%chg']\n",
    "df_cumsum = df_returns_all_warrants.set_index('date')[plot_col_names].cumsum()\n",
    "min_date = df_cumsum[~df_cumsum['open_close_t+1_%chg'].isna()].index.min()\n",
    "df_cumsum = df_cumsum[df_cumsum.index>=min_date]\n",
    "fig = go.Figure()\n",
    "fig.update_layout(title='Cumulative Return Trading on All 8-Ks: SPAC Warrants',\n",
    "                  xaxis_title='Date',\n",
    "                  yaxis_title='Cumulative Return in Percentage Points (10 = 1000%)')\n",
    "for plot_col_name in plot_col_names:\n",
    "    fig.add_trace(go.Scatter(x=df_cumsum.index,\n",
    "                             y=df_cumsum[plot_col_name],\n",
    "                             mode='lines+markers',\n",
    "                             marker=dict(size=3),\n",
    "                             name=plot_col_name))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_returns_all_warrants_minute.sort_values(by='accepted_time', inplace=True)\n",
    "plt.figure(figsize=(10,6));\n",
    "# df_returns_all_warrants_minute.set_index('accepted_time')['accepted_time_t+1hr_%chg'].cumsum().plot(label='accepted_time+1hr')\n",
    "# df_returns_all_warrants_minute.set_index('accepted_time')['accepted_time_t+3hr_%chg'].cumsum().plot(label='accepted_time+3hr')\n",
    "# df_returns_all_warrants_minute.set_index('accepted_time')['accepted_time_t+5hr_%chg'].cumsum().plot(label='accepted_time+5hr')\n",
    "# df_returns_all_warrants_minute.set_index('accepted_time')['accepted_time_t+7hr_%chg'].cumsum().plot(label='accepted_time+7hr')\n",
    "# df_returns_all_warrants_minute.set_index('accepted_time')['accepted_time_t+10hr_%chg'].cumsum().plot(label='accepted_time+10hr')\n",
    "# df_returns_all_warrants_minute.set_index('accepted_time')['accepted_time_t+12hr_%chg'].cumsum().plot(label='accepted_time+12hr')\n",
    "df_returns_all_warrants_minute.set_index('date')['accepted_time_t+1_%chg'].cumsum().plot(label='accepted_time+1day')\n",
    "df_returns_all_warrants_minute.set_index('date')['accepted_time_t+3_%chg'].cumsum().plot(label='accepted_time+3day')\n",
    "df_returns_all_warrants_minute.set_index('date')['accepted_time_t+5_%chg'].cumsum().plot(label='accepted_time+5day')\n",
    "df_returns_all_warrants_minute.set_index('date')['accepted_time_t+7_%chg'].cumsum().plot(label='accepted_time+7day')\n",
    "df_returns_all_warrants_minute.set_index('date')['accepted_time_t+10_%chg'].cumsum().plot(label='accepted_time+10day')\n",
    "df_returns_all_warrants_minute.set_index('date')['accepted_time_t+12_%chg'].cumsum().plot(label='accepted_time+12day')\n",
    "plt.title('Cumulative Return on All 8-Ks with Minute Data, All SPAC Warrants');\n",
    "plt.ylabel('Cumulative Return in Percentage Points (10 = 1000%)');\n",
    "plt.xticks(rotation=45);\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict warrants return using 8-K content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_header_footer(text):\n",
    "    # remove/replace some unicode characters\n",
    "    text = text.replace('\\t','')\n",
    "    text = text.replace('\\x93','\"')\n",
    "    text = text.replace('\\x94','\"')\n",
    "    text = text.replace('', '\"') # weird unicode/ascii conversion issue\n",
    "    text = text.replace('', '\"') # weird unicode/ascii conversion issue\n",
    "    \n",
    "    # remove everything in header and footer\n",
    "    ind_start = text.find('financial accounting standards provided pursuant to section 13(a) of the exchange act')\n",
    "    ind_end = text.find('signature pursuant to the requirements of the securities exchange act of 1934')\n",
    "    text = text[ind_start:ind_end]\n",
    "    \n",
    "    # remove forward looking statement section\n",
    "    FLS_START = [\n",
    "    'forward-looking statements this current report',\n",
    "    'forward looking statements certain statements',\n",
    "    'forward-looking statements this report',\n",
    "    'forward-looking statements the company makes',\n",
    "    'forward-looking statements certain of the matters',\n",
    "    'forward-looking statements this communication',\n",
    "    'forward-looking statements this document'\n",
    "    ]\n",
    "    FLS_END = [\n",
    "    'undue reliance should not be placed upon the forward-looking statements',\n",
    "    'whether as a result of new information, future events or otherwise, except as required by law',\n",
    "    'conditions or circumstances on which any such statement is based, except as required by applicable law',\n",
    "    'whether as a result of new information, future events, or otherwise'\n",
    "    ]\n",
    "    for forward_start in FLS_START:\n",
    "        for forward_end in FLS_END:\n",
    "            ind_start = text.find(forward_start)\n",
    "            ind_end = text.find(forward_end)\n",
    "            if ind_start!=-1 and ind_end!=-1:\n",
    "                text = text[0:ind_start] + text[ind_end+len(forward_end):]\n",
    "    \n",
    "    # additional text to remove\n",
    "    text_to_remove = [\n",
    "        'financial accounting standards provided pursuant to section 13(a) of the exchange act'\n",
    "    ]\n",
    "    for rm in text_to_remove:\n",
    "        text = text.replace(rm,'')\n",
    "    \n",
    "    return text\n",
    "\n",
    "def get_item_subheaders(text, subheaders_only):\n",
    "    subheaders = re.findall('item [0-9]+\\.[0-9]+', text)\n",
    "    subheaders = list(collections.OrderedDict.fromkeys(subheaders)) # handle cases where subheader mentioned in content\n",
    "    subtexts = []\n",
    "    for i in range(0,len(subheaders)):\n",
    "        if i+1==len(subheaders):\n",
    "            subtext = text[text.find(subheaders[i]):]\n",
    "        else:\n",
    "            subtext = text[text.find(subheaders[i]):text.find(subheaders[i+1])]\n",
    "        if 'item 9.01 financial statements and exhibits' in subtext:\n",
    "            continue\n",
    "        else:\n",
    "            subtexts.append(subtext)\n",
    "    \n",
    "    # drop these items\n",
    "    drop_item_list = ['item 9.01']\n",
    "    \n",
    "    if subheaders_only:\n",
    "        return [x for x in subheaders if x not in drop_item_list]\n",
    "    return subtexts\n",
    "\n",
    "def count_keywords(x, keywords):\n",
    "    count = 0\n",
    "    for keyword in keywords:\n",
    "        count = count + x.count(keyword)\n",
    "    return count\n",
    "\n",
    "def text_processing(text, item_features, stemming=True):\n",
    "    subtexts = get_item_subheaders(text, subheaders_only=False)\n",
    "    tokens = []\n",
    "    for subtext in subtexts:\n",
    "        # remove item subheaders. todo: probably should remove text of subheader too\n",
    "        for item in item_features:\n",
    "            subtext = subtext.replace(item,'')\n",
    "\n",
    "        # tokenize, only keeping alphanumeric\n",
    "        subtokens = nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(subtext)\n",
    "\n",
    "        # remove stop words\n",
    "        stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "        subtokens = [w for w in subtokens if not w in stop_words]\n",
    "\n",
    "        # remove numbers and chinese/other non-english characters for now\n",
    "        # (todo: revisit later for vote count processing)\n",
    "        subtokens = [w for w in subtokens if w.encode('utf-8').isalpha()]\n",
    "\n",
    "        # stemming\n",
    "        if stemming:\n",
    "            stems = [nltk.stem.porter.PorterStemmer().stem(w) for w in subtokens]\n",
    "            tokens.extend(stems)\n",
    "        else:\n",
    "            tokens.extend(subtokens)\n",
    "\n",
    "    processed_text = ' '.join(tokens)\n",
    "    return processed_text\n",
    "\n",
    "def add_subheader_item_features(df_ret, item_features):\n",
    "    # item definitions: https://www.sec.gov/fast-answers/answersform8khtm.html\n",
    "    # item faq: https://media2.mofo.com/documents/faq-form-8-k.pdf\n",
    "    for col in item_features:\n",
    "        df_ret[col] = 0\n",
    "    for i in range(0, len(df_ret)):\n",
    "        text = df_ret.loc[i]['text']\n",
    "        items = get_item_subheaders(text, subheaders_only=True)\n",
    "        for item in items:\n",
    "            df_ret.loc[i,item] = 1\n",
    "    return df_ret\n",
    "\n",
    "def add_bagofwords_features(df_ret, vectorizer_type, response_variable):\n",
    "    # process text and create bigram, trigram features\n",
    "    corpus = []\n",
    "    for text in df_ret['text']:\n",
    "        corpus.append(text_processing(text))\n",
    "    print('vectorizer:', vectorizer_type, '\\n')\n",
    "    if vectorizer_type=='CountVectorizer':\n",
    "        vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 3)) # Total Frequency\n",
    "    elif vectorizer_type=='TfidfVectorizer':\n",
    "        vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(2, 3)) # TF-IDF\n",
    "    else:\n",
    "        print('vectorizer_type d.n.e')\n",
    "    X_corpus = vectorizer.fit_transform(corpus)\n",
    "    df_bagofwords = pd.DataFrame(X_corpus.toarray())\n",
    "    df_bagofwords.columns = vectorizer.get_feature_names()\n",
    "    \n",
    "    # concatenate\n",
    "    df_ret = pd.concat([df_ret, df_bagofwords], axis=1)\n",
    "\n",
    "    # example features\n",
    "    print('letter intent features:', [x for x in df_bagofwords.columns if 'letter intent' in x])\n",
    "\n",
    "    # histogram\n",
    "    (n, bins, patches) = plt.hist(df_ret[response_variable], bins=10, label=response_variable)\n",
    "    plt.title('Histogram of ' + response_variable)\n",
    "    plt.show()\n",
    "    print('bins', np.round(bins,2))\n",
    "    print('counts', n)\n",
    "    \n",
    "    return df_ret\n",
    "\n",
    "def convert_vote_count_to_int(x):\n",
    "    if '' in x or '-' in x or 'n/a' in x:\n",
    "        return 0\n",
    "    try:\n",
    "        x = int(x.replace(',',''))\n",
    "    except:\n",
    "        return np.nan\n",
    "    return x\n",
    "\n",
    "# does not handle: https://www.sec.gov/Archives/edgar/data/1704760/000161577419006723/s117785_8k.htm (no broker non-votes)\n",
    "def parse_vote_results(x):\n",
    "    # find strings in text. use first if multiple matches\n",
    "    VOTE_HEADER = [\n",
    "        'for against abstain broker non-votes',\n",
    "        'for against abstain broker non-vote',\n",
    "        'for against abstention broker non-votes',\n",
    "        'for against abstention broker non-vote',\n",
    "        'for against abstentions broker non-votes',\n",
    "        'for against abstentions broker non-vote'\n",
    "    ]\n",
    "    # find phrases preceding vote results in text\n",
    "    vote_strings = [vote_string for vote_string in VOTE_HEADER if vote_string in x['text']]\n",
    "\n",
    "    # parse votes for, votes against, votes abstain, votes broker non votes\n",
    "    if len(vote_strings)==0:\n",
    "        return pd.Series([np.nan, np.nan, np.nan, np.nan])\n",
    "    else:\n",
    "        vote_string = vote_strings[0] # use first if multiple matches\n",
    "        vote_index = x['text'].find(vote_string)\n",
    "        vote_data = x['text'][(vote_index + len(vote_string)):].lstrip().split(' ')\n",
    "        votes_for = convert_vote_count_to_int(vote_data[0])\n",
    "        votes_against = convert_vote_count_to_int(vote_data[1])\n",
    "        votes_abstain = convert_vote_count_to_int(vote_data[2])\n",
    "        votes_broker_non_votes = convert_vote_count_to_int(vote_data[3])\n",
    "        if np.isnan(votes_for) or np.isnan(votes_against) or np.isnan(votes_abstain) or np.isnan(votes_broker_non_votes):\n",
    "            print('something wrong with parse_vote_results for', x.symbol, 'on', x.date)\n",
    "        return pd.Series([votes_for, votes_against, votes_abstain, votes_broker_non_votes])\n",
    "    return pd.Series([np.nan, np.nan, np.nan, np.nan])\n",
    "\n",
    "def parse_redemptions(x):\n",
    "    REDEMPTION_HEADER = [\n",
    "        'in connection with the extension',\n",
    "        'in connection with the closing',\n",
    "        'in advance of the special meeting',\n",
    "        'in connection with the special meeting',\n",
    "        'exercised their right'\n",
    "    ]\n",
    "    for redemption_phrase in REDEMPTION_HEADER:\n",
    "        redemption_sentence = [sentence for sentence in x.text.split('.') if\n",
    "                               redemption_phrase in sentence\n",
    "                               and ('redeem' in sentence or 'redemp' in sentence)]\n",
    "        if len(redemption_sentence)>0:\n",
    "            break\n",
    "    if len(redemption_sentence)==0:\n",
    "        return np.nan\n",
    "    redemption_sentence = redemption_sentence[0].lstrip().replace(',','')\n",
    "    redemption_sentence = re.sub(r'[\\$]{1}[\\d,]+\\.?\\d{0,2}', '', redemption_sentence) # replace $_#_\n",
    "    shares_regex_strong = re.findall('[0-9]+ shares', redemption_sentence)\n",
    "    shares_regex_weak = re.findall('[0-9]+', redemption_sentence)\n",
    "    # case: just one number in sentence. assume this number is redemption number\n",
    "    if len(shares_regex_weak)==1:\n",
    "        shares = int(shares_regex_weak[0])\n",
    "    # case: more than one number in sentence. if '[0-9]+ shares' then assume this is redemption number, otherwise nan\n",
    "    elif len(shares_regex_weak)>1:\n",
    "        if len(shares_regex_strong)==1:\n",
    "            shares = int(shares_regex_strong[0].replace('shares','').replace(' ',''))\n",
    "        else:\n",
    "            shares = np.nan\n",
    "    # case: no numbers in sentence.\n",
    "    else:\n",
    "        if 'none' in redemption_sentence:\n",
    "            shares = 0\n",
    "        else:\n",
    "            shares = np.nan\n",
    "    return shares\n",
    "\n",
    "def compute_self_engineered_feature_metrics(df_ret, col_name, response_variable):\n",
    "    print('count', col_name, '> 0:', np.sum(df_ret[col_name]>0),\n",
    "          '; count', response_variable, '> 0:', len(df_ret[df_ret[col_name]>0][df_ret[response_variable]>0]),\n",
    "          '; count unique symbols:', len(df_ret[df_ret[col_name]>0]['symbol'].unique()))\n",
    "\n",
    "def add_self_engineered_features(df_ret, response_variable=None):    \n",
    "    # keywords lists\n",
    "    keywords_list_loi = [\n",
    "    'entry into a letter of intent',\n",
    "    'entry into a non-binding letter of intent',\n",
    "    'enter into a letter of intent',\n",
    "    'enter into a non-binding letter of intent',\n",
    "    'entered into a letter of intent',\n",
    "    'entered into a non-binding letter of intent',\n",
    "    'entering into a letter of intent',\n",
    "    'entering into a non-binding letter of intent',\n",
    "    'execution of a letter of intent',\n",
    "    'execution of a non-binding letter of intent',\n",
    "    'execute a letter of intent',\n",
    "    'execute a non-binding letter of intent',\n",
    "    'executed a letter of intent',\n",
    "    'executed a non-binding letter of intent',\n",
    "    'executing a letter of intent',\n",
    "    'executing a non-binding letter of intent'\n",
    "    ]\n",
    "    keywords_list_business_combination_agreement = [\n",
    "    '(\"business combination agreement\")',\n",
    "    '(the \"business combination agreement\")',\n",
    "    '(\"business combination\")',\n",
    "    '(the \"business combination\")',\n",
    "    'entry into a definitive agreement',\n",
    "    'enter into a definitive agreement',\n",
    "    'entered into a definitive agreement',\n",
    "    'entering into a definitive agreement'\n",
    "    ]\n",
    "    keywords_list_merger_agreement = ['(the \"merger agreement\")']\n",
    "    keywords_list_purchase_agreement = ['(the \"purchase agreement\")']\n",
    "    keywords_list_extension = [\n",
    "    '(the \"extension\")',\n",
    "    '(the \"extension amendment\")',\n",
    "    'extended the termination date',\n",
    "    'extend the date by which the company must consummate',\n",
    "    'extend the date by which the company has to complete',\n",
    "    '(the \"extension amendment proposal\")',\n",
    "    '(the \"extended termination date\")'\n",
    "    ]\n",
    "    keywords_list_meeting = ['(\"special meeting\")', '(the \"meeting\")']\n",
    "    keywords_list_record = ['(the \"record date\")']\n",
    "    keywords_list_consummation = [\n",
    "    'announcing the consummation',\n",
    "    'consummated the previously announced business combination'\n",
    "    ]\n",
    "    keywords_list_ipo = [\n",
    "    'consummated its initial public offering (\"ipo\")',\n",
    "    'consummated its initial public offering (the \"ipo\")',\n",
    "    'consummated an initial public offering (\"ipo\")',\n",
    "    'consummated an initial public offering (the \"ipo\")',\n",
    "    'consummated the initial public offering (\"ipo\")',\n",
    "    'consummated the initial public offering (the \"ipo\")',\n",
    "    'completed its initial public offering (\"ipo\")',\n",
    "    'completed its initial public offering (the \"ipo\")',\n",
    "    'in connection with its initial public offering (\"ipo\") was declared effective',\n",
    "    'in connection with its initial public offering (the \"ipo\") was declared effective',\n",
    "    'consummated the ipo',\n",
    "    'in connection with the closing of the ipo',\n",
    "    'closing of the initial public offering (the \"ipo\")',\n",
    "    'consummation of the ipo'\n",
    "    ]\n",
    "    keywords_list_trust = [\n",
    "    'trust account'\n",
    "    ]\n",
    "    \n",
    "    # keywords counts\n",
    "    df_ret['keywords_loi'] = df_ret.text.apply(lambda x: count_keywords(x, keywords_list_loi))\n",
    "    df_ret['keywords_business_combination_agreement'] = df_ret.text.apply(lambda x: count_keywords(x, keywords_list_business_combination_agreement))\n",
    "    df_ret['keywords_merger_agreement'] = df_ret.text.apply(lambda x: count_keywords(x, keywords_list_merger_agreement))\n",
    "    df_ret['keywords_purchase_agreement'] = df_ret.text.apply(lambda x: count_keywords(x, keywords_list_purchase_agreement))\n",
    "    df_ret['keywords_extension'] = df_ret.text.apply(lambda x: count_keywords(x, keywords_list_extension))\n",
    "    df_ret['keywords_meeting'] = df_ret.text.apply(lambda x: count_keywords(x, keywords_list_meeting))\n",
    "    df_ret['keywords_record'] = df_ret.text.apply(lambda x: count_keywords(x, keywords_list_record))\n",
    "    df_ret['keywords_consummation'] = df_ret.text.apply(lambda x: count_keywords(x, keywords_list_consummation))\n",
    "    df_ret['keywords_ipo'] = df_ret.text.apply(lambda x: count_keywords(x, keywords_list_ipo))\n",
    "    df_ret['keywords_trust'] = df_ret.text.apply(lambda x: count_keywords(x, keywords_list_trust))\n",
    "    \n",
    "    # compute metrics\n",
    "    if response_variable is not None:\n",
    "        keywords_name_list = ['keywords_loi','keywords_business_combination_agreement','keywords_merger_agreement',\n",
    "                              'keywords_purchase_agreement','keywords_extension','keywords_meeting','keywords_record',\n",
    "                              'keywords_consummation','keywords_ipo','keywords_trust','item 5.07','item 3.01','item 2.03']\n",
    "        for keywords_name in keywords_name_list:\n",
    "            compute_self_engineered_feature_metrics(df_ret, keywords_name, response_variable)\n",
    "\n",
    "    # vote results\n",
    "    df_ret['votes_for'] = np.nan\n",
    "    df_ret['votes_against'] = np.nan\n",
    "    df_ret['votes_abstain'] = np.nan\n",
    "    df_ret['votes_broker_non_votes'] = np.nan\n",
    "    df_ret[['votes_for', 'votes_against', 'votes_abstain', 'votes_broker_non_votes']] = df_ret.apply(lambda x: parse_vote_results(x), axis=1)\n",
    "    df_ret['vote_total'] = df_ret['votes_for'] + df_ret['votes_against'] + df_ret['votes_abstain'] + df_ret['votes_broker_non_votes']\n",
    "    df_ret['%votes_for'] = df_ret['votes_for'] / df_ret['vote_total']\n",
    "    df_ret['%vote_against'] = df_ret['votes_against'] / df_ret['vote_total']\n",
    "    df_ret['%votes_abstain'] = df_ret['votes_abstain'] / df_ret['vote_total']\n",
    "    df_ret['%votes_broker_non_votes'] = df_ret['votes_broker_non_votes'] / df_ret['vote_total']\n",
    "    \n",
    "    # shares redeemed\n",
    "    df_ret['redeemed_shares'] = df_ret.apply(lambda x: parse_redemptions(x), axis=1)\n",
    "    df_ret['%redeemed'] = df_ret['redeemed_shares'] / df_ret['vote_total']\n",
    "    \n",
    "    return df_ret\n",
    "\n",
    "# Useful text:\n",
    "# \"Item 7.01 Regulation FD Disclosure\": LOI, Business Combination Agreement, Business Combination consummation\n",
    "# \"Item 5.07 Submission of Matters to a Vote of Security Holders\": vote extension, vote result\n",
    "# key words LOI: definitive agreement, letter of intent\n",
    "# key words business combination agreement: \"business combination agreement\", \"business combination\", \"merger agreement\", \"purchase agreement\"\n",
    "# key words vote: (\"special meeting\"), (the \"meeting\"), (the \"extension\"), (the \"extension amendment\"), (the \"record date\")\n",
    "# key words other: (the \"closing date\")\n",
    "\n",
    "# Not useful:\n",
    "# dropped \"item 9.01 financial statements and exhibits\"\n",
    "# maybe drop \"Forward-Looking Statements\", \"No Offer or Solicitation\"\n",
    "# maybe drop \"Item 5.02. Departure of Directors or Certain Officers; Election of Directors; Appointment of Certain Officers; Compensatory Arrangements of Certain Officers.\"\n",
    "\n",
    "# other notes:\n",
    "# 8-Ks aren't actually iid, maybe symbols with previous positive reactions are more likely to positive\n",
    "#    - example of bad reaction KXIN Form 25 https://www.sec.gov/Archives/edgar/data/1713539/000135445719000234/0001354457-19-000234-index.htm\n",
    "#    - another example: NSCO Form 25 https://www.sec.gov/Archives/edgar/data/1709682/000087666119000802/0000876661-19-000802-index.htm\n",
    "# stratify by sector\n",
    "# add class weight instead of cv with f1 score\n",
    "# maybe create set of features using all legal terms in (), e.g. (the \"special meeting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data processing inputs\n",
    "y_variable = 'open_close_t+5_%chg' # t+1, t+2,..., or t+12\n",
    "\n",
    "# get all warrants and drop nan and corrupt symbols\n",
    "# todo: merge newticker and oldticker df_returns_past_warrants properly. use newticker for now\n",
    "df_returns_warrants = df_returns_past_warrants_newticker.append(df_returns_current_warrants)\n",
    "df_returns_warrants.drop(columns=['letter_of_intent_found',\n",
    "                                  'business_combination_agreement_found',\n",
    "                                  'form','open_completion_%chg'], inplace=True)\n",
    "\n",
    "print('count 8-Ks:', len(df_returns_warrants))\n",
    "df_returns_warrants.dropna(inplace=True)\n",
    "print('count 8-Ks after dropping nan prices:', len(df_returns_warrants))\n",
    "# remove corrupt symbols\n",
    "df_returns_warrants = df_returns_warrants[~df_returns_warrants.symbol.isin(['ACELW','LAZYW'])]\n",
    "print('count 8-Ks after dropping corrupt symbols:', len(df_returns_warrants), '\\n')\n",
    "df_returns_warrants.sort_values(by='accepted_time', inplace=True) # need to sort for TimeSeriesSplit\n",
    "df_returns_warrants.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print('predicting:', y_variable)\n",
    "\n",
    "# define item features\n",
    "item_features = ['item 1.01','item 1.02','item 1.03','item 1.04','item 2.01','item 2.02','item 2.03','item 2.04',\n",
    "                 'item 2.05','item 2.06','item 3.01','item 3.02','item 3.03','item 4.01','item 4.02','item 5.01',\n",
    "                 'item 5.02','item 5.03','item 5.04','item 5.05','item 5.06','item 5.07','item 5.08','item 6.01',\n",
    "                 'item 6.02','item 6.03','item 6.04','item 6.05','item 7.01','item 8.01']\n",
    "\n",
    "# remove header and footer\n",
    "df_returns_warrants['text'] = df_returns_warrants.text.apply(lambda x: remove_header_footer(x))\n",
    "\n",
    "# add subheader item binary features\n",
    "df_returns_warrants = add_subheader_item_features(df_ret=df_returns_warrants, item_features=item_features)\n",
    "\n",
    "# add bag of words features\n",
    "# df_returns_warrants = add_bagofwords_features(df_ret=df_returns_warrants,\n",
    "#                                               vectorizer_type='CountVectorizer', # CountVectorizer or TfidfVectorizer\n",
    "#                                               response_variable=y_variable)\n",
    "\n",
    "# add self engineered features\n",
    "df_returns_warrants = add_self_engineered_features(df_ret=df_returns_warrants, response_variable=y_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# study different keywords\n",
    "\n",
    "def string_match_test(test_strings):\n",
    "    print('strings match:', test_strings, '\\n')\n",
    "    df_matched = df_returns_warrants[(df_returns_warrants.text.str.contains('|'.join(test_strings))) &\n",
    "                                     (df_returns_warrants['item 2.03']==0)]\n",
    "    print('count 8-Ks matched:', len(df_matched))\n",
    "    df_matched.set_index('date')['open_close_t+1_%chg'].cumsum().plot(label='Open-Close T+1')\n",
    "    df_matched.set_index('date')['open_close_t+3_%chg'].cumsum().plot(label='Open-Close T+3')\n",
    "    df_matched.set_index('date')['open_close_t+5_%chg'].cumsum().plot(label='Open-Close T+5')\n",
    "    df_matched.set_index('date')['open_close_t+7_%chg'].cumsum().plot(label='Open-Close T+7')\n",
    "    df_matched.set_index('date')['open_close_t+10_%chg'].cumsum().plot(label='Open-Close T+10')\n",
    "    df_matched.set_index('date')['open_close_t+12_%chg'].cumsum().plot(label='Open-Close T+12')\n",
    "    plt.ylabel('Cumulative Return in Percentage Points (10 = 1000%)');\n",
    "    plt.xticks(rotation=45);\n",
    "    plt.legend();\n",
    "    plt.show();\n",
    "\n",
    "# loi\n",
    "string_match_test(test_strings=[\n",
    "    'entry into a letter of intent',\n",
    "    'entry into a non-binding letter of intent',\n",
    "    'entered into a letter of intent',\n",
    "    'entered into a non-binding letter of intent',\n",
    "    'entering into a letter of intent',\n",
    "    'entering into a non-binding letter of intent',\n",
    "    'execute a letter of intent',\n",
    "    'execute a non-binding letter of intent',\n",
    "    'executed a letter of intent',\n",
    "    'executed a non-binding letter of intent',\n",
    "    'executing a letter of intent',\n",
    "    'executing a non-binding letter of intent',\n",
    "    'execution of a letter of intent',\n",
    "    'execution of a non-binding letter of intent',\n",
    "])\n",
    "    \n",
    "# bca\n",
    "string_match_test(test_strings=[\n",
    "    '(\"business combination agreement\")',\n",
    "    '(the \"business combination agreement\")',\n",
    "    '(\"business combination\")',\n",
    "    '(the \"business combination\")',\n",
    "    'entry into a definitive agreement',\n",
    "    'enter into a definitive agreement',\n",
    "    'entered into a definitive agreement',\n",
    "    'entering into a definitive agreement',\n",
    "    'business combination proposal was approved',\n",
    "#     'entered into a stock and unit purchase agreement',\n",
    "#     'entered into a securities purchase agreement',\n",
    "#     'entered into an agreement and plan of merger',\n",
    "#     'entered into a share exchange agreement',\n",
    "])\n",
    "\n",
    "# extension\n",
    "keywords_list_extension = [\n",
    "'(the \"extension\")',\n",
    "'(the \"extension amendment\")',\n",
    "'extended the termination date',\n",
    "'extend the date by which the company must consummate',\n",
    "'extend the date by which the company has to complete',\n",
    "'(the \"extension amendment proposal\")',\n",
    "'(the \"extended termination date\")'\n",
    "]\n",
    "string_match_test(test_strings=keywords_list_extension)\n",
    "\n",
    "# consummation\n",
    "string_match_test(test_strings=[\n",
    "    'announcing the consummation',\n",
    "    'consummated the previously announced business combination',\n",
    "    'consummated the remainder of the transactions contemplated by the business combination',\n",
    "    'consummation of the transactions',\n",
    "])\n",
    "                  \n",
    "# investor presentation\n",
    "string_match_test(test_strings=[\n",
    "    'form of presentation',\n",
    "    'furnished an investor presentation',\n",
    "    'updated investor presentation',\n",
    "    'investor presentation are furnished',\n",
    "    'form of investor presentation',\n",
    "    'update the investor presentation',\n",
    "])\n",
    "string_match_test(test_strings=[ # this could be a good one\n",
    "    'investor presentation'\n",
    "])\n",
    "\n",
    "# trust account\n",
    "string_match_test(test_strings=[\n",
    "    '(the \"trust account\")',\n",
    "    '(the \"contribution\")',\n",
    "    '(the \"revised contribution\")',\n",
    "])\n",
    "string_match_test(test_strings=[ # this could be a good one\n",
    "    'trust account',\n",
    "])\n",
    "\n",
    "# form 25-nse\n",
    "string_match_test(test_strings=[ # expect this to be a sell signal\n",
    "    'form 25',\n",
    "])\n",
    "\n",
    "# consummate IPO\n",
    "keywords_list_ipo = [\n",
    "'consummated its initial public offering (\"ipo\")',\n",
    "'consummated its initial public offering (the \"ipo\")',\n",
    "'consummated an initial public offering (\"ipo\")',\n",
    "'consummated an initial public offering (the \"ipo\")',\n",
    "'consummated the initial public offering (\"ipo\")',\n",
    "'consummated the initial public offering (the \"ipo\")',\n",
    "'completed its initial public offering (\"ipo\")',\n",
    "'completed its initial public offering (the \"ipo\")',\n",
    "'in connection with its initial public offering (\"ipo\") was declared effective',\n",
    "'in connection with its initial public offering (the \"ipo\") was declared effective',\n",
    "'consummated the ipo',\n",
    "'in connection with the closing of the ipo',\n",
    "'closing of the initial public offering (the \"ipo\")',\n",
    "'consummation of the ipo'\n",
    "]\n",
    "string_match_test(test_strings=keywords_list_ipo)\n",
    "\n",
    "# pipe investor\n",
    "string_match_test(test_strings=[\n",
    "    'pipe',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly sample 8-Ks for populating test cases\n",
    "# path_saved_8K = 'data/sec_filings_df'\n",
    "# files_8Ks = [f for f in os.listdir(path_saved_8K) if os.path.isfile(os.path.join(path_saved_8K, f))]\n",
    "# df_8Ks = pd.DataFrame()\n",
    "# for file in files_8Ks:\n",
    "#     df_8K = pd.read_csv(path_saved_8K+'/'+file)\n",
    "#     df_8K['symbol'] = file.split('_')[0]\n",
    "#     df_8Ks = df_8Ks.append(df_8K)\n",
    "# df_8Ks.drop_duplicates(subset=['date','accepted_time','text'], inplace=True)\n",
    "# df_8Ks.reset_index(drop=True, inplace=True)\n",
    "# R = np.random.RandomState(123)\n",
    "# random_inds = R.random_integers(0, len(df_8Ks), 110)\n",
    "# df_8Ks[df_8Ks.index.isin(random_inds[100:110])]\n",
    "\n",
    "# check single test case\n",
    "# spac_list_temp = process_current_spacs(file_path_current='data/spac_list_temp.csv', write=False)\n",
    "# temp = get_forms_text(company_name=spac_list_temp.title[0], cik_id=spac_list_temp.cik[0], form_type='8-K')\n",
    "# temp['symbol'] = spac_list_temp.ticker[0]\n",
    "# temp['text'] = temp.text.apply(lambda x: remove_header_footer(x))\n",
    "# temp = add_subheader_item_features(df_ret=temp, item_features=item_features)\n",
    "# temp = add_self_engineered_features(df_ret=temp)\n",
    "# display(temp.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lsa_dim_reduction(X, n_lsa):\n",
    "    lsa = TruncatedSVD(n_components=n_lsa, n_iter=10, random_state=123)\n",
    "    X = pd.DataFrame(lsa.fit_transform(X), columns=['lsa'+str(i) for i in range(0,n_lsa)])\n",
    "    print('count feature after LSA:', len(X.columns), '\\n')\n",
    "    return X\n",
    "\n",
    "def binary_classification_eval_metrics(y_actual, y_pred):\n",
    "    print('% predicted positive labels:', np.round(sum(y_pred)/len(y_pred), 2))\n",
    "    print('% actual positive labels:', np.round(sum(y_actual)/len(y_actual), 2))\n",
    "    cm = confusion_matrix(y_actual, y_pred)\n",
    "    print('confusion matrix:\\n', cm)\n",
    "    print(classification_report(y_actual, y_pred, target_names=['0','1']))\n",
    "\n",
    "def binary_classification_report(model, X_train, X_test, y_train, y_test, feature_importance=True):\n",
    "    print('############\\nTraining set\\n############')\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    binary_classification_eval_metrics(y_train, y_train_pred)\n",
    "    metrics.plot_roc_curve(model, X_train, y_train);\n",
    "    plt.title('ROC curve (train)')\n",
    "    plt.show()\n",
    "    metrics.plot_precision_recall_curve(model, X_train, y_train)\n",
    "    plt.title('Precision-recall curve (train)');\n",
    "    plt.show()\n",
    "    \n",
    "    print('#########\\nTest set\\n#########')\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    binary_classification_eval_metrics(y_test, y_test_pred)\n",
    "    metrics.plot_roc_curve(model, X_test, y_test);\n",
    "    plt.title('ROC curve (test)')\n",
    "    plt.show()\n",
    "    metrics.plot_precision_recall_curve(model, X_test, y_test)\n",
    "    plt.title('Precision-recall curve (test)');\n",
    "    plt.show()\n",
    "    \n",
    "    print('#########\\nAll data\\n#########')\n",
    "    y_all_pred = model.predict(X)\n",
    "    binary_classification_eval_metrics(y, y_all_pred)\n",
    "    metrics.plot_roc_curve(model, X, y);\n",
    "    plt.title('ROC curve (all)')\n",
    "    plt.show()\n",
    "    metrics.plot_precision_recall_curve(model, X, y)\n",
    "    plt.title('Precision-recall curve (all)');\n",
    "    plt.show()\n",
    "    \n",
    "    # feature importance\n",
    "    if feature_importance:\n",
    "        try:\n",
    "            coefs = model.coef_[0]\n",
    "        except:\n",
    "            coefs = model.feature_importances_\n",
    "        indices = np.argsort(np.abs(coefs))[::-1]\n",
    "        count_nonzero_features = len(X.columns[coefs!=0])\n",
    "        print('count nonzero features:', count_nonzero_features)\n",
    "        if count_nonzero_features < 10:\n",
    "            top_n_features = count_nonzero_features\n",
    "        else:\n",
    "            top_n_features = 10\n",
    "        plt.bar(range(top_n_features), coefs[indices[:top_n_features]], align='center')\n",
    "        plt.xticks(range(top_n_features), X.columns[indices[:top_n_features]], rotation=45, ha='right')\n",
    "        plt.subplots_adjust(bottom=0.3)\n",
    "        plt.title('Feature importance')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "label_threshold = 0\n",
    "min_word_freq = 0\n",
    "n_lsa = None # None or int\n",
    "\n",
    "# prep data\n",
    "cols_drop = ['symbol','date','accepted_time','text','url','votes_for','votes_against',\n",
    "             'votes_abstain','votes_broker_non_votes','vote_total','%votes_for',\n",
    "             '%votes_abstain','%votes_broker_non_votes','redeemed_shares']\n",
    "cols_drop.extend([col for col in df_returns_warrants.columns if '%chg' in col])\n",
    "X = df_returns_warrants.drop(cols_drop, axis=1)\n",
    "X[['%vote_against','%redeemed']] = X[['%vote_against','%redeemed']].fillna(0)\n",
    "print('count features:', len(X.columns))\n",
    "X = X[X.columns[X.sum() >= min_word_freq].tolist()] # min word frequency\n",
    "print('count features > min_word_freq:', len(X.columns))\n",
    "if n_lsa is not None:\n",
    "    X = apply_lsa_dim_reduction(X, n_lsa) # lsa dimension reduction\n",
    "y = np.where(df_returns_warrants[y_variable] > label_threshold, 1, 0) # label threshold\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "print('training dim:', X_train.shape)\n",
    "print('testing dim:', X_test.shape, '\\n')\n",
    "print('count 0:', np.sum(y==0))\n",
    "print('count 1:', np.sum(y==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Naive rule based model\\n')\n",
    "\n",
    "def naive_rule(x):\n",
    "    if ~np.isnan(x['%vote_against']) and (x['%vote_against'] > .10):\n",
    "        return 0\n",
    "    elif x['keywords_ipo'] > 0:\n",
    "        return 0\n",
    "    else:\n",
    "        if (x['keywords_loi'] > 0) and (x['item 2.03'] == 0):\n",
    "            return 1\n",
    "        elif (x['keywords_business_combination_agreement'] > 0) and (x['item 2.03'] == 0):\n",
    "            return 1\n",
    "        elif (x['keywords_consummation'] > 0) and (x['item 2.03'] == 0):\n",
    "            return 1\n",
    "        elif (x['keywords_extension'] > 0) and (x['item 2.03'] == 0):\n",
    "            return 1\n",
    "        elif (x['keywords_trust'] > 0) and (x['item 2.03'] == 0):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "y_train_pred = X_train.apply(lambda x: naive_rule(x), axis=1)\n",
    "y_test_pred = X_test.apply(lambda x: naive_rule(x), axis=1)\n",
    "y_all_pred = X.apply(lambda x: naive_rule(x), axis=1)\n",
    "\n",
    "print('############\\nTraining set\\n############')\n",
    "binary_classification_eval_metrics(y_train, y_train_pred)\n",
    "\n",
    "print('#########\\nTest set\\n#########')\n",
    "binary_classification_eval_metrics(y_test, y_test_pred)\n",
    "\n",
    "print('#########\\nAll data\\n#########')\n",
    "binary_classification_eval_metrics(y, y_all_pred)\n",
    "print('Count examples:', len(y))\n",
    "print('Count positive labels:', y_all_pred.sum())\n",
    "count_months = int(np.round((dt.strptime(df_returns_warrants.date.max(),'%Y-%m-%d') -\n",
    "                             dt.strptime(df_returns_warrants.date.min(),'%Y-%m-%d')).days / 365. * 12, 0))\n",
    "print('Count months:', count_months)\n",
    "print('Trades / month:', np.round(y_all_pred.sum()/count_months, 1))\n",
    "\n",
    "df_pred_pos = df_returns_warrants.loc[np.where(y_all_pred==1)[0],]\n",
    "print(y_variable, 'sum:', np.round(df_pred_pos[y_variable].sum(), 2))\n",
    "# todo: calculate sharpe and sortino ratios properly\n",
    "print(y_variable, 'mean:', np.round(df_pred_pos[y_variable].mean(), 2))\n",
    "print(y_variable, 'sd:', np.round(df_pred_pos[y_variable].std(), 2))\n",
    "print(y_variable, 'downside sd:', np.round(df_pred_pos[y_variable][df_pred_pos[y_variable]<0].std(), 2))\n",
    "\n",
    "df_pred_pos.set_index('date')['open_close_t+1_%chg'].cumsum().plot(label='Open-Close T+1')\n",
    "df_pred_pos.set_index('date')['open_close_t+3_%chg'].cumsum().plot(label='Open-Close T+3')\n",
    "df_pred_pos.set_index('date')['open_close_t+5_%chg'].cumsum().plot(label='Open-Close T+5')\n",
    "df_pred_pos.set_index('date')['open_close_t+7_%chg'].cumsum().plot(label='Open-Close T+7')\n",
    "df_pred_pos.set_index('date')['open_close_t+10_%chg'].cumsum().plot(label='Open-Close T+10')\n",
    "df_pred_pos.set_index('date')['open_close_t+12_%chg'].cumsum().plot(label='Open-Close T+12')\n",
    "plt.title('Cumulative Return on Predicted Positive Labels, All Warrants');\n",
    "plt.ylabel('Cumulative Return in Percentage Points (10 = 1000%)');\n",
    "plt.xticks(rotation=45);\n",
    "plt.legend();\n",
    "plt.show();\n",
    "\n",
    "cols_plot = ['keywords_loi',\n",
    "#              'keywords_business_combination_agreement',\n",
    "#              'keywords_consummation',\n",
    "             'keywords_extension',\n",
    "             'keywords_trust']\n",
    "for col in cols_plot:\n",
    "    df_pred_pos[df_pred_pos[col]>0].set_index('date')['open_close_t+1_%chg'].cumsum().plot(label='Open-Close T+1')\n",
    "    df_pred_pos[df_pred_pos[col]>0].set_index('date')['open_close_t+3_%chg'].cumsum().plot(label='Open-Close T+3')\n",
    "    df_pred_pos[df_pred_pos[col]>0].set_index('date')['open_close_t+5_%chg'].cumsum().plot(label='Open-Close T+5')\n",
    "    df_pred_pos[df_pred_pos[col]>0].set_index('date')['open_close_t+7_%chg'].cumsum().plot(label='Open-Close T+7')\n",
    "    df_pred_pos[df_pred_pos[col]>0].set_index('date')['open_close_t+10_%chg'].cumsum().plot(label='Open-Close T+10')\n",
    "    df_pred_pos[df_pred_pos[col]>0].set_index('date')['open_close_t+12_%chg'].cumsum().plot(label='Open-Close T+12')\n",
    "    plt.title('Cumulative Return on Predicted Positive Labels, All Warrants, '+col);\n",
    "    plt.ylabel('Cumulative Return in Percentage Points (10 = 1000%)');\n",
    "    plt.xticks(rotation=45);\n",
    "    plt.legend();\n",
    "    plt.show();\n",
    "    \n",
    "# gini-like metric for predicted positive labels\n",
    "keywords_name_list = ['keywords_loi','keywords_extension','keywords_trust']\n",
    "for keywords_name in keywords_name_list:\n",
    "    compute_self_engineered_feature_metrics(df_pred_pos, keywords_name, y_variable)\n",
    "\n",
    "# todo: weird ones to look at\n",
    "# GTYH: df_returns_warrants[df_returns_warrants.symbol=='GTYHW'].loc[115,'text']...somehow doesn't match up with https://www.sec.gov/Archives/edgar/data/1682325/000114420419014675/tv516340_8k12ba.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print('loi symbols:', df_returns_warrants[(df_returns_warrants.keywords_loi>0)].symbol.unique())\n",
    "# print('\\nbusiness combination agreement symbols:', df_returns_warrants[(df_returns_warrants.keywords_business_combination_agreement>0)].symbol.unique())\n",
    "# print('\\nextension symbols:', df_returns_warrants[(df_returns_warrants.keywords_extension>0)].symbol.unique())\n",
    "# print('\\nconsummation symbols:', df_returns_warrants[(df_returns_warrants.keywords_consummation>0)].symbol.unique())\n",
    "# print('\\ntrust symbols:', df_returns_warrants[(df_returns_warrants.keywords_trust>0)].symbol.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_pred_pos.groupby('symbol').agg({'accepted_time':'count'}).sort_values(by='accepted_time', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df_pred_pos.sort_values(by='open_close_t+3_%chg').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Logistic Regression\\n')\n",
    "\n",
    "# CV\n",
    "model_lr = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000)\n",
    "C = [.01, .1, 1, 10, 100, 1000]\n",
    "# C = np.linspace(.5, 1.5, 10)\n",
    "param_grid = dict(C=C)\n",
    "kfold = TimeSeriesSplit(n_splits=5) # in each split test indices must be higher than before\n",
    "grid_search = GridSearchCV(model_lr, param_grid, scoring='precision', cv=kfold, verbose=0, n_jobs=5)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "print('Best 5-fold CV (TimeSeriesSplit) score: %f using %s' %\n",
    "      (grid_result.best_score_, grid_result.best_params_), '\\n')\n",
    "reg_path = pd.DataFrame({'score': grid_result.cv_results_['mean_test_score'], 'C': C})\n",
    "reg_path['C'] = np.log10(reg_path['C'])\n",
    "reg_path.plot(x='C')\n",
    "plt.title('CV path')\n",
    "plt.xlabel('log10(C)')\n",
    "plt.show()\n",
    "\n",
    "# model\n",
    "model_lr = LogisticRegression(penalty='l1', solver='liblinear', C=grid_result.best_params_['C'], max_iter=1000)\n",
    "model_lr.fit(X_train, y_train)\n",
    "binary_classification_report(model_lr, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Decision Tree\\n')\n",
    "\n",
    "# CV\n",
    "model_dt = DecisionTreeClassifier()\n",
    "max_depth = list(range(1, 20))\n",
    "# min_samples_split = [.001, .01, .05, .1]\n",
    "# min_samples_leaf = [.001, .01, .05, .1]\n",
    "param_grid = dict(max_depth=max_depth) #, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)\n",
    "kfold = TimeSeriesSplit(n_splits=5) # in each split test indices must be higher than before\n",
    "grid_search = GridSearchCV(model_dt, param_grid, scoring='f1', cv=kfold, verbose=0, n_jobs=5)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "print('Best 5-fold CV (TimeSeriesSplit) score: %f using %s' %\n",
    "      (grid_result.best_score_, grid_result.best_params_), '\\n')\n",
    "reg_path = pd.DataFrame({'score': grid_result.cv_results_['mean_test_score'], 'max_depth': max_depth})\n",
    "reg_path.plot(x='max_depth')\n",
    "plt.title('CV path')\n",
    "plt.xlabel('max_depth')\n",
    "plt.show()\n",
    "\n",
    "# model\n",
    "model_dt = DecisionTreeClassifier(max_depth=grid_result.best_params_['max_depth'])\n",
    "#                                   min_samples_split=grid_result.best_params_['min_samples_split'],\n",
    "#                                   min_samples_leaf=grid_result.best_params_['min_samples_leaf'])\n",
    "model_dt.fit(X_train, y_train)\n",
    "binary_classification_report(model_dt, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# tree diagram\n",
    "tree_data = export_graphviz(model_dt, out_file=None, feature_names=X.columns, class_names=['-1','1'],\n",
    "                            filled=True, rounded=True, rotate=True, impurity=False)\n",
    "graph = graphviz.Source(tree_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Random Forest\\n')\n",
    "\n",
    "# CV\n",
    "model_rf = RandomForestClassifier()\n",
    "n_estimators = list(range(1, 5))\n",
    "max_depth = list(range(1, 5))\n",
    "min_samples_leaf = [.01, .05, .1, .2]\n",
    "max_features = [.01, .05, .1]\n",
    "param_grid = dict(n_estimators=n_estimators, max_depth=max_depth,\n",
    "                  min_samples_leaf=min_samples_leaf, max_features=max_features)\n",
    "kfold = TimeSeriesSplit(n_splits=5) # in each split test indices must be higher than before\n",
    "grid_search = GridSearchCV(model_rf, param_grid, scoring='f1', cv=kfold, verbose=0, n_jobs=5)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "print('Best 5-fold CV (TimeSeriesSplit) score: %f using %s' %\n",
    "      (grid_result.best_score_, grid_result.best_params_), '\\n')\n",
    "\n",
    "# model\n",
    "model_rf = RandomForestClassifier(n_estimators=grid_result.best_params_['n_estimators'],\n",
    "                                  max_depth=grid_result.best_params_['max_depth'],\n",
    "                                  min_samples_leaf=grid_result.best_params_['min_samples_leaf'],\n",
    "                                  max_features=grid_result.best_params_['max_features'])\n",
    "model_rf.fit(X_train, y_train)\n",
    "binary_classification_report(model_rf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('SVM\\n')\n",
    "\n",
    "kernel_type = 'linear' # linear, rbf, poly\n",
    "\n",
    "# CV\n",
    "model_svm = SVC(kernel=kernel_type)\n",
    "C = [.01, .1, 1, 10, 100, 1000, 10000]\n",
    "# C = [.6, .7, .8, .9, 1, 1.1, 1.2]\n",
    "param_grid = dict(C=C)\n",
    "kfold = TimeSeriesSplit(n_splits=5) # in each split test indices must be higher than before\n",
    "grid_search = GridSearchCV(model_svm, param_grid, scoring='f1', cv=kfold, verbose=0, n_jobs=5)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "print('Best 5-fold CV (TimeSeriesSplit) score: %f using %s' %\n",
    "      (grid_result.best_score_, grid_result.best_params_), '\\n')\n",
    "reg_path = pd.DataFrame({'score': grid_result.cv_results_['mean_test_score'], 'C': C})\n",
    "reg_path['C'] = np.log10(reg_path['C'])\n",
    "reg_path.plot(x='C')\n",
    "plt.title('CV path')\n",
    "plt.xlabel('log10(C)')\n",
    "plt.show()\n",
    "\n",
    "# model\n",
    "model_svm = SVC(kernel=kernel_type, C=grid_result.best_params_['C'])\n",
    "model_svm.fit(X_train, y_train)\n",
    "binary_classification_report(model_svm, X_train, X_test, y_train, y_test, feature_importance=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-Class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multi_labels(x):\n",
    "    if x < first_label_threshold:\n",
    "        return -1\n",
    "    elif x > second_label_threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def plot_roc_multi(model_multi, X_input, y_input):\n",
    "    y_binarized = preprocessing.label_binarize(y_input, classes=[-1, 0, 1])\n",
    "    n_classes = y_binarized.shape[1]\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    from itertools import cycle\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = metrics.roc_curve(y_binarized[:, i], model_multi.predict_proba(X_input)[:, i])\n",
    "        roc_auc[i] = metrics.auc(fpr[i], tpr[i])\n",
    "    colors = cycle(['blue', 'red', 'green'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=1,\n",
    "                 label='ROC curve of class {0} (AUC = {1:0.2f})'\n",
    "                 ''.format(i-1, roc_auc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "    plt.xlim([-0.05, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "def multi_classification_eval_metrics(y_actual, y_pred):\n",
    "    print('% predicted (actual) -1 labels: {} ({})'.format(np.round(sum(y_pred==-1)/len(y_pred), 2),\n",
    "                                                           np.round(sum(y_actual==-1)/len(y_actual), 2)))\n",
    "    print('% predicted (actual) 0 labels: {} ({})'.format(np.round(sum(y_pred==0)/len(y_pred), 2),\n",
    "                                                          np.round(sum(y_actual==0)/len(y_actual), 2)))\n",
    "    print('% predicted (actual) 1 labels: {} ({})'.format(np.round(sum(y_pred==1)/len(y_pred), 2),\n",
    "                                                          np.round(sum(y_actual==1)/len(y_actual), 2)))\n",
    "    cm = confusion_matrix(y_actual, y_pred)\n",
    "    print('confusion matrix:\\n', cm)\n",
    "    print(classification_report(y_actual, y_pred, target_names=['-1','0','1']))\n",
    "    \n",
    "def multi_classification_report(model_multi, X_train, X_test, y_train, y_test, feature_importance=True):\n",
    "    print('############\\nTraining set\\n############')\n",
    "    y_pred_train = model_multi.predict(X_train)\n",
    "    multi_classification_eval_metrics(y_train, y_pred_train)\n",
    "    plot_roc_multi(model_multi, X_train, y_train)\n",
    "\n",
    "    print('#########\\nTest set\\n#########')\n",
    "    y_pred = model_multi.predict(X_test)\n",
    "    multi_classification_eval_metrics(y_test, y_pred)\n",
    "    plot_roc_multi(model_multi, X_test, y_test)\n",
    "\n",
    "    # feature importance\n",
    "    if feature_importance:\n",
    "        try:\n",
    "            coefs = model_multi.coef_[0]\n",
    "        except:\n",
    "            coefs = model_multi.feature_importances_\n",
    "        indices = np.argsort(np.abs(coefs))[::-1]\n",
    "        print('count nonzero features:', len(X.columns[coefs!=0]))\n",
    "        top_n_features = 20 #len(X.columns[coefs!=0])\n",
    "        plt.bar(range(top_n_features), coefs[indices[:top_n_features]], align='center')\n",
    "        plt.xticks(range(top_n_features), X.columns[indices[:top_n_features]], rotation=45, ha='right')\n",
    "        plt.subplots_adjust(bottom=0.3)\n",
    "        plt.title('Feature importance')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "first_label_threshold = -0.05 #-0.17\n",
    "second_label_threshold = 0.05 #0.38\n",
    "min_word_freq = 0\n",
    "n_lsa = None\n",
    "\n",
    "# prep data\n",
    "X = df_returns_warrants.drop([y_variable,'symbol','date','text'], axis=1)\n",
    "print('count features:', len(X.columns))\n",
    "X = X[X.columns[X.sum() >= min_word_freq].tolist()] # min word frequency\n",
    "print('count features > min_word_freq:', len(X.columns))\n",
    "if n_lsa is not None:\n",
    "    X = apply_lsa_dim_reduction(X, n_lsa) # lsa dimension reduction\n",
    "y = df_returns_warrants[y_variable].map(get_multi_labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "print('training dim:', X_train.shape)\n",
    "print('testing dim:', X_test.shape, '\\n')\n",
    "print('count -1:', np.sum(y==-1))\n",
    "print('count 0:', np.sum(y==0))\n",
    "print('count 1:', np.sum(y==1), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Logistic Regression\\n')\n",
    "\n",
    "# CV\n",
    "model_lr_multi = LogisticRegression(penalty='l1', solver='saga', multi_class='multinomial')\n",
    "C = [.01, .1, 1, 10, 50, 100, 1000]\n",
    "param_grid = dict(C=C)\n",
    "kfold = TimeSeriesSplit(n_splits=5) # in each split test indices must be higher than before\n",
    "grid_search = GridSearchCV(model_lr_multi, param_grid, scoring=metrics.make_scorer(metrics.f1_score, average='macro'),\n",
    "                           cv=kfold, verbose=0, n_jobs=5)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "print('Best 5-fold CV (TimeSeriesSplit) score: %f using %s' % (grid_result.best_score_, grid_result.best_params_))\n",
    "reg_path = pd.DataFrame({'score': grid_result.cv_results_['mean_test_score'], 'C': C})\n",
    "reg_path['C'] = np.log10(reg_path['C'])\n",
    "reg_path.plot(x='C')\n",
    "plt.title('CV path')\n",
    "plt.xlabel('log10(C)')\n",
    "plt.show()\n",
    "\n",
    "# model\n",
    "model_lr_multi = LogisticRegression(penalty='l1', solver='saga', multi_class='multinomial',\n",
    "                                    C=grid_result.best_params_['C'])\n",
    "model_lr_multi.fit(X_train, y_train)\n",
    "multi_classification_report(model_lr_multi, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Decision Tree\\n')\n",
    "\n",
    "# CV\n",
    "model_dt_multi = DecisionTreeClassifier()\n",
    "max_depth = list(range(1, 10))\n",
    "param_grid = dict(max_depth=max_depth)\n",
    "kfold = TimeSeriesSplit(n_splits=5) # in each split test indices must be higher than before\n",
    "grid_search = GridSearchCV(model_dt_multi, param_grid, scoring='f1_macro', cv=kfold, verbose=0, n_jobs=5)\n",
    "y_train_binarized = preprocessing.label_binarize(y_train, classes=[-1, 0, 1])\n",
    "grid_result = grid_search.fit(X_train, y_train_binarized)\n",
    "print('Best 5-fold CV (TimeSeriesSplit) score: %f using %s' %\n",
    "      (grid_result.best_score_, grid_result.best_params_), '\\n')\n",
    "reg_path = pd.DataFrame({'score': grid_result.cv_results_['mean_test_score'], 'max_depth': max_depth})\n",
    "reg_path.plot(x='max_depth')\n",
    "plt.title('CV path')\n",
    "plt.xlabel('max_depth')\n",
    "plt.show()\n",
    "\n",
    "# model\n",
    "model_dt_multi = DecisionTreeClassifier(max_depth=grid_result.best_params_['max_depth'])\n",
    "model_dt_multi.fit(X_train, y_train)\n",
    "multi_classification_report(model_dt_multi, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# tree diagram\n",
    "# tree_data = export_graphviz(model_dt_multi, out_file=None, feature_names=X.columns, class_names=['-1','0','1'],\n",
    "#                             filled=True, rounded=True, rotate=True, impurity=False)\n",
    "# graph = graphviz.Source(tree_data)\n",
    "# graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Random Forest\\n')\n",
    "\n",
    "# CV\n",
    "model_rf_multi = RandomForestClassifier()\n",
    "n_estimators = list(range(1, 10))\n",
    "max_depth = list(range(1, 5))\n",
    "min_samples_leaf = [.01, .05, .1, .2, .3]\n",
    "max_features = [.05, .25, .5, .75]\n",
    "param_grid = dict(n_estimators=n_estimators, max_depth=max_depth,\n",
    "                  min_samples_leaf=min_samples_leaf, max_features=max_features)\n",
    "kfold = TimeSeriesSplit(n_splits=5) # in each split test indices must be higher than before\n",
    "grid_search = GridSearchCV(model_rf_multi, param_grid, scoring='f1_macro', cv=kfold, verbose=0, n_jobs=5)\n",
    "y_train_binarized = preprocessing.label_binarize(y_train, classes=[-1, 0, 1])\n",
    "grid_result = grid_search.fit(X_train, y_train_binarized)\n",
    "print('Best 5-fold CV (TimeSeriesSplit) score: %f using %s' %\n",
    "      (grid_result.best_score_, grid_result.best_params_), '\\n')\n",
    "\n",
    "# model\n",
    "model_rf_multi = RandomForestClassifier(n_estimators=grid_result.best_params_['n_estimators'],\n",
    "                                        max_depth=grid_result.best_params_['max_depth'],\n",
    "                                        min_samples_leaf=grid_result.best_params_['min_samples_leaf'],\n",
    "                                        max_features=grid_result.best_params_['max_features'])\n",
    "model_rf_multi.fit(X_train, y_train)\n",
    "multi_classification_report(model_rf_multi, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('SVM\\n')\n",
    "\n",
    "kernel_type = 'linear' # linear, rbf, poly\n",
    "\n",
    "# CV\n",
    "model_svm_multi = SVC(kernel=kernel_type, probability=True)\n",
    "C = [.01, .1, 1, 10, 50, 100, 150, 1000, 10000]\n",
    "param_grid = dict(C=C)\n",
    "kfold = TimeSeriesSplit(n_splits=5) # in each split test indices must be higher than before\n",
    "grid_search = GridSearchCV(model_svm_multi, param_grid, scoring='f1_macro', cv=kfold, verbose=0, n_jobs=5)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "print('Best 5-fold CV (TimeSeriesSplit) score: %f using %s' %\n",
    "      (grid_result.best_score_, grid_result.best_params_), '\\n')\n",
    "reg_path = pd.DataFrame({'score': grid_result.cv_results_['mean_test_score'], 'C': C})\n",
    "reg_path['C'] = np.log10(reg_path['C'])\n",
    "reg_path.plot(x='C')\n",
    "plt.title('CV path')\n",
    "plt.xlabel('log10(C)')\n",
    "plt.show()\n",
    "\n",
    "# model\n",
    "model_svm_multi = SVC(kernel=kernel_type, C=grid_result.best_params_['C'], probability=True)\n",
    "model_svm_multi.fit(X_train, y_train)\n",
    "multi_classification_report(model_svm_multi, X_train, X_test, y_train, y_test, feature_importance=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
